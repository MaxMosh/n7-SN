{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e18462-faf1-4b25-b5cb-c371c2a7914b",
   "metadata": {},
   "source": [
    "# Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f56f00-ecf0-4add-a755-80d58026c784",
   "metadata": {},
   "source": [
    "This notebook has been heavily inspired by the excellent video \"Seq. 07 / Pytorch\" from the FIDLE training (formation FIDLE) : <br> https://www.youtube.com/watch?v=brktdGzMHN8 <br> We highly encourage you to go watch this video after the Programming Practical if you want to deepen your knowledge about Pytorch ! &#128521;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb47c6-1acb-4063-894d-e189487b1b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:34.406635Z",
     "iopub.status.busy": "2024-03-29T10:10:34.406536Z",
     "iopub.status.idle": "2024-03-29T10:10:35.514175Z",
     "shell.execute_reply": "2024-03-29T10:10:35.513595Z",
     "shell.execute_reply.started": "2024-03-29T10:10:34.406624Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "print(\"Pytorch version is\", torch.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a4225-a2cb-4f11-9aa5-b45c58ae93b6",
   "metadata": {},
   "source": [
    "# Pytorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0cbac-0685-4645-81b3-e0b4ab8629f3",
   "metadata": {},
   "source": [
    "Tensors behave kind of like numpy arrays, except that they can be used to propagate a gradient and can be easily transfered to a GPU (but we won't use that last property in this PP) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c373a-0e45-4c9b-a8fd-1857cc57da68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.515251Z",
     "iopub.status.busy": "2024-03-29T10:10:35.514949Z",
     "iopub.status.idle": "2024-03-29T10:10:35.523723Z",
     "shell.execute_reply": "2024-03-29T10:10:35.523369Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.515238Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor([[1, 2],[-3,-4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c437d1a-131c-4dd7-a2b2-7eb11d9fa1f7",
   "metadata": {},
   "source": [
    "You can easily precise the type of the variables in your tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e0550-218a-4d75-8d80-bfae2cb88114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.524272Z",
     "iopub.status.busy": "2024-03-29T10:10:35.524149Z",
     "iopub.status.idle": "2024-03-29T10:10:35.555333Z",
     "shell.execute_reply": "2024-03-29T10:10:35.555024Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.524252Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.zeros([3,5], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2e313-1c82-446a-b30f-784e66fb3bb3",
   "metadata": {},
   "source": [
    "And you can also easily access the data it contains !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d08a2-953d-47cd-acf7-40769c60a3af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.555918Z",
     "iopub.status.busy": "2024-03-29T10:10:35.555792Z",
     "iopub.status.idle": "2024-03-29T10:10:35.558591Z",
     "shell.execute_reply": "2024-03-29T10:10:35.558247Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.555908Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.tensor([[1, 2],[-3,-4]])\n",
    "print(A[0,1])\n",
    "print(A[0,1].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c8388-abbf-4f9c-a6e6-df25f071ed06",
   "metadata": {},
   "source": [
    "For convenience, tensors can also be converted to numpy arrays (such as when you want to use matplotlib for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ab9132-42a0-4d22-85fa-6488baf2000b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.559152Z",
     "iopub.status.busy": "2024-03-29T10:10:35.559039Z",
     "iopub.status.idle": "2024-03-29T10:10:35.568346Z",
     "shell.execute_reply": "2024-03-29T10:10:35.568024Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.559142Z"
    }
   },
   "outputs": [],
   "source": [
    "A.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9c468-053c-438d-875f-47d50032bbd4",
   "metadata": {},
   "source": [
    "You can also transpose tensors, with torch.transpose(tensor, first_dim_to_transpose, second_dim_to_transpose). You can also use .T, but be careful with the dimensions if you are in more than 2D !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc1030e-b9f5-41c9-aed0-8945a21e6934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.568925Z",
     "iopub.status.busy": "2024-03-29T10:10:35.568784Z",
     "iopub.status.idle": "2024-03-29T10:10:35.580912Z",
     "shell.execute_reply": "2024-03-29T10:10:35.580579Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.568915Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.randn([3,4,5])\n",
    "print(A)\n",
    "print(A.shape)\n",
    "A_t = torch.transpose(A,0,1)\n",
    "print(A_t.shape)\n",
    "A_t = torch.transpose(A,0,2)\n",
    "print(A_t.shape)\n",
    "print(A.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e1d41c-a643-441a-8c38-4512f4b50c1b",
   "metadata": {},
   "source": [
    "You can reshape the tensors with view() and flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d236a-b75f-42ad-91d8-c7588bcbfe78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.582280Z",
     "iopub.status.busy": "2024-03-29T10:10:35.582160Z",
     "iopub.status.idle": "2024-03-29T10:10:35.587711Z",
     "shell.execute_reply": "2024-03-29T10:10:35.587388Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.582270Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.randint(0,5,[3,4])\n",
    "print(A)\n",
    "print(A.view([6,2]))\n",
    "print(A.view([-1, 1]))\n",
    "print(A.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af8b11-6b2a-48b5-a367-c05cfc19b580",
   "metadata": {},
   "source": [
    "You can concatenate tensors in several ways, but be careful with the dimensions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225cd7e-3793-4cf0-a885-36edb8d128d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.588309Z",
     "iopub.status.busy": "2024-03-29T10:10:35.588163Z",
     "iopub.status.idle": "2024-03-29T10:10:35.597205Z",
     "shell.execute_reply": "2024-03-29T10:10:35.596866Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.588299Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.randn([3, 3])\n",
    "B = torch.randn([5, 3])\n",
    "\n",
    "print(torch.vstack((A,B)))\n",
    "print(torch.hstack((A, B.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabb0f1d-027d-49ab-bd30-f438af4d5896",
   "metadata": {},
   "source": [
    "# Linear algebra with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13784b41-e625-440a-9a63-0cc368751cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T09:01:08.422826Z",
     "iopub.status.busy": "2023-10-26T09:01:08.422193Z",
     "iopub.status.idle": "2023-10-26T09:01:08.427990Z",
     "shell.execute_reply": "2023-10-26T09:01:08.427507Z",
     "shell.execute_reply.started": "2023-10-26T09:01:08.422772Z"
    }
   },
   "source": [
    "You can perform basic linear algebra operations with tensors ! Let's begin with norm calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b6431-2d2d-4fe8-ad95-7ae34874dad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.597826Z",
     "iopub.status.busy": "2024-03-29T10:10:35.597667Z",
     "iopub.status.idle": "2024-03-29T10:10:35.607561Z",
     "shell.execute_reply": "2024-03-29T10:10:35.606530Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.597815Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(5)\n",
    "print(x)\n",
    "print(torch.norm(x, p = 2)) #2-norm\n",
    "print(torch.norm(x, p = 1)) #1-norm\n",
    "print(torch.norm(x, p = float('inf'))) #inf-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d06db-9894-4e7f-9c17-4b3aca988738",
   "metadata": {},
   "source": [
    "You can also perform dot products (inner and outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0f671-11b5-4a6f-901c-f8c05b6f0ef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.608532Z",
     "iopub.status.busy": "2024-03-29T10:10:35.608268Z",
     "iopub.status.idle": "2024-03-29T10:10:35.620828Z",
     "shell.execute_reply": "2024-03-29T10:10:35.620361Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.608509Z"
    }
   },
   "outputs": [],
   "source": [
    "y = torch.randn(5)\n",
    "print(torch.dot(x, y))\n",
    "print(torch.inner(x, y))\n",
    "print(torch.outer(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d09401-a162-4502-876e-8039000be930",
   "metadata": {},
   "source": [
    "But be careful, the * corresponds to elementwise product !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc99b1a-f37c-47e7-9aa7-4c62d021738e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.621992Z",
     "iopub.status.busy": "2024-03-29T10:10:35.621458Z",
     "iopub.status.idle": "2024-03-29T10:10:35.627297Z",
     "shell.execute_reply": "2024-03-29T10:10:35.626974Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.621958Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293daea-c18c-4434-9d04-be4fc1517fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-26T12:39:23.710844Z",
     "iopub.status.busy": "2023-10-26T12:39:23.710261Z",
     "iopub.status.idle": "2023-10-26T12:39:23.714682Z",
     "shell.execute_reply": "2023-10-26T12:39:23.714271Z",
     "shell.execute_reply.started": "2023-10-26T12:39:23.710800Z"
    }
   },
   "source": [
    "Of course, you can perform matrix multiplication as well !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d4f47-c2ad-4840-88b3-903b92cd076c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.627867Z",
     "iopub.status.busy": "2024-03-29T10:10:35.627718Z",
     "iopub.status.idle": "2024-03-29T10:10:35.637350Z",
     "shell.execute_reply": "2024-03-29T10:10:35.637045Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.627856Z"
    }
   },
   "outputs": [],
   "source": [
    "A = torch.randn(5, 5)\n",
    "B = torch.randn(5, 2)\n",
    "\n",
    "print(torch.matmul(A, B))\n",
    "print(A @ B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82de699-446d-4954-a9f6-bd5a2ad10133",
   "metadata": {},
   "source": [
    "# Automatic differentiation with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070ee0bc-50a4-4454-ac79-cf8ce64316f5",
   "metadata": {},
   "source": [
    "Now the main advantage of Pytorch tensors, automatic differentiation ! Automatic differentiation refers to a set of techniques to evaluate the partial derivative of a function specified by a computer program. In our case, the program is a Neural Network. To use automatic differentiation in Pytorch, you have to use the requires_grad = True argument when you create your tensor. Make sure your tensor is a tensor of floats ! Gradients can only be required for floats or complexs tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d110d3b-b67c-4a45-b1f7-ba3ffd206ecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.637931Z",
     "iopub.status.busy": "2024-03-29T10:10:35.637769Z",
     "iopub.status.idle": "2024-03-29T10:10:35.644694Z",
     "shell.execute_reply": "2024-03-29T10:10:35.644366Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.637920Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(4., requires_grad = True)\n",
    "y = torch.tensor(2.)\n",
    "z = x*y\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)\n",
    "print(z.requires_grad)\n",
    "\n",
    "y.requires_grad = True\n",
    "\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853745e3-a4e2-42fe-a910-0353ff12d8a7",
   "metadata": {},
   "source": [
    "When you create a tensor directly, Pytorch calls it a leaf tensor (or leaf node), but if you create a tensor by composing other tensors, it becomes a non-leaf tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127a87a-5c3d-4ed3-bb4e-256c32018957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.645271Z",
     "iopub.status.busy": "2024-03-29T10:10:35.645120Z",
     "iopub.status.idle": "2024-03-29T10:10:35.653363Z",
     "shell.execute_reply": "2024-03-29T10:10:35.652888Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.645259Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.is_leaf)\n",
    "print(y.is_leaf)\n",
    "print(z.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d9a90-b507-4fc9-a086-3e7cd3d1a30e",
   "metadata": {},
   "source": [
    "The gradient is stocked in the .grad argument of the tensor. For the moment, the gradient is None because no backpropagation/backward operation has been done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1b10e-f445-4e26-8564-b83c577cf3bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.654242Z",
     "iopub.status.busy": "2024-03-29T10:10:35.654044Z",
     "iopub.status.idle": "2024-03-29T10:10:35.662886Z",
     "shell.execute_reply": "2024-03-29T10:10:35.662480Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.654224Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e36533-a3cc-4cde-9d3d-2803638ab1aa",
   "metadata": {},
   "source": [
    "Be careful ! If you want to track the gradient of a non-leaf tensor, you have to use the .retain_grad() method after creating the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892f124-df40-4474-ab7a-3faed8476332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.663745Z",
     "iopub.status.busy": "2024-03-29T10:10:35.663467Z",
     "iopub.status.idle": "2024-03-29T10:10:35.671952Z",
     "shell.execute_reply": "2024-03-29T10:10:35.671535Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.663726Z"
    }
   },
   "outputs": [],
   "source": [
    "z.retain_grad()\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf6854-8ff9-4b0e-95f2-a412871fe333",
   "metadata": {},
   "source": [
    "To populate the gradient argument, you have to perform a backward pass with the .backward() method on the function you want to calculate the gradient of (here the function output is called loss). The .backward() method will run through every variables that were involved in the evaluation of the function (here x and y but not z) and will populate their .grad argument with the gradient of the function with respect to that variable. If you want to backawrd through the graph multiple times (needed for PINNs), you have to add the reatin_graph = True argument, otherwise the graph values will be discarded after each backward pass for memory issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda9515-4980-4d1f-9ee0-f3eced636982",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.672552Z",
     "iopub.status.busy": "2024-03-29T10:10:35.672406Z",
     "iopub.status.idle": "2024-03-29T10:10:35.686546Z",
     "shell.execute_reply": "2024-03-29T10:10:35.686133Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.672541Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = x**2 + y**2\n",
    "loss.backward(retain_graph = True)\n",
    "\n",
    "print(loss)\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e095d43-03e7-43ec-b45d-e7fd81c6fbf4",
   "metadata": {},
   "source": [
    "CAREFUL : if you call .backward() several times without zeroing the gradients in between, the gradients will accumulate in the .grad argument of the variables. Zeroing the gradients is done with a simple command when using an optimizer, as we will see later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961bd9c-fbab-40f3-a50b-04573ce96592",
   "metadata": {},
   "source": [
    "You can also use the no_grad() function if you want to perform operations for which you don't want Pytorch to trace the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb1487f-127a-4c54-9020-7718bdd5208a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:10:35.687104Z",
     "iopub.status.busy": "2024-03-29T10:10:35.686996Z",
     "iopub.status.idle": "2024-03-29T10:10:35.689804Z",
     "shell.execute_reply": "2024-03-29T10:10:35.689424Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.687093Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    loss = x**2 + y**2\n",
    "\n",
    "print(loss) # The grad_fn=<AddBackward0> argument which was visible before is now gone.\n",
    "    \n",
    "# loss.backward(retain_graph = True) # Uncommenting this line will produce a RuntimeError. Try it once then comment the line again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4305ac-cdb6-46c8-8b84-e2204fba48fd",
   "metadata": {},
   "source": [
    "You can also use the autograd.grad() function, which will also run through every variables needed to perform the evaluation of the function, as the .backward() method, but it will concatenate the different components to get the gradient vector at the end. The syntax of autograd.grad() is (function_to_calculate_the_gradient_of, variables_to_calculate_the_gradient_from). Since the autograd.grad function returns a tuple of tensors, you need to precise [0] to take only the first value of the tuple, which is the gradient we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea04f419-c082-4b4a-8321-d2eb57765888",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:13:56.741824Z",
     "iopub.status.busy": "2024-03-29T10:13:56.741491Z",
     "iopub.status.idle": "2024-03-29T10:13:56.754897Z",
     "shell.execute_reply": "2024-03-29T10:13:56.754411Z",
     "shell.execute_reply.started": "2024-03-29T10:13:56.741807Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = torch.hstack((x,y))\n",
    "loss = parameters[0]**2 + parameters[1]**2\n",
    "\n",
    "gradient = torch.autograd.grad(loss, parameters)[0]\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc88593-728e-4945-b135-6884db39437f",
   "metadata": {},
   "source": [
    "If you want to calculate higher orders derivatives (needed for PINNs), you need to specify create_graph = True as well !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb6ca9-db24-471c-8e19-d5952af9bde3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:14:03.070021Z",
     "iopub.status.busy": "2024-03-29T10:14:03.069723Z",
     "iopub.status.idle": "2024-03-29T10:14:03.074561Z",
     "shell.execute_reply": "2024-03-29T10:14:03.074232Z",
     "shell.execute_reply.started": "2024-03-29T10:14:03.069994Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = torch.hstack((x,y))\n",
    "loss = parameters[0]**2 + parameters[1]**2\n",
    "\n",
    "gradient = torch.autograd.grad(loss, parameters,\n",
    "                               retain_graph = True,\n",
    "                               create_graph = True)[0]\n",
    "\n",
    "Hessian = torch.zeros(gradient.shape[0], gradient.shape[0])\n",
    "for i in range (gradient.shape[0]):\n",
    "    Hessian[:, i] = torch.autograd.grad(gradient[i], parameters,\n",
    "                                        retain_graph = True)[0].flatten()\n",
    "print(Hessian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d3bfb-41f6-46f9-9193-1cde06b162ec",
   "metadata": {},
   "source": [
    "If you want to compute the gradient of a loss function evaluated at several points (used for PINNs), you have to precise the grad_outputs option to tell Pytorch the format in which you expect the gradient to be. Otherwise, Pytorch thinks you want to compute the full Jacobian matrix of the vector containing all the loss values with respect to the parameters, which is not the case. You need to inform Pytorch there is a one-to-one correspondance between each row of parameter and each compoennt of the loss, by setting grad_outputs = torch.ones_like(the vector containting the values of the evaluatiions of the function you want to compute the gradient of)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c067db83-9dd8-4fa6-8fd3-86e185e18aa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-29T10:29:34.806805Z",
     "iopub.status.busy": "2024-03-29T10:29:34.804958Z",
     "iopub.status.idle": "2024-03-29T10:29:34.811723Z",
     "shell.execute_reply": "2024-03-29T10:29:34.811428Z",
     "shell.execute_reply.started": "2024-03-29T10:29:34.806720Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = torch.vstack((torch.hstack((x,y)), 3*torch.hstack((x,y)), 2*torch.hstack((x,y))))\n",
    "loss = torch.norm(parameters, p = 2, dim = 1)**2\n",
    "\n",
    "gradient = torch.autograd.grad(loss, parameters,\n",
    "                               grad_outputs = torch.ones_like(loss),\n",
    "                               retain_graph = True,\n",
    "                               create_graph = True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029bf40-1c37-43a2-986f-c9ff5ce2226a",
   "metadata": {},
   "source": [
    "# Optimization with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b6ebf-6a0c-4ec2-bbf3-62d8cc085457",
   "metadata": {},
   "source": [
    "Optimizing with Pytorch is very easy ! As you will see, you only need to define an optimizer and then everything works fluently from that ! First let's define a basic function and a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc6b5b6b-31c6-4c5c-8fc0-666904c19131",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.716683Z",
     "iopub.status.idle": "2024-03-29T10:10:35.716818Z",
     "shell.execute_reply": "2024-03-29T10:10:35.716754Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.716747Z"
    }
   },
   "outputs": [],
   "source": [
    "list_xy = []\n",
    "\n",
    "x = torch.tensor(-3., requires_grad = True)\n",
    "y = torch.tensor(3., requires_grad = True)\n",
    "\n",
    "def f(x, y):\n",
    "    return (1-x)**2 + 100*(y-x**2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89e651-0c3b-4038-aa8a-18443656160f",
   "metadata": {},
   "source": [
    "Now let's define the optimizer ! Most PINNs use a deterministic gradient descent method like L-BFGS because stochastic gradient descent converges less precisely, thus leading in huge errors in the physical solution. Since PINNs are mainly used with relatively low number of colocation points, the computation of the full gradient is affordable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "136d15de-e35d-498f-ac38-4881411649f3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.717309Z",
     "iopub.status.idle": "2024-03-29T10:10:35.717719Z",
     "shell.execute_reply": "2024-03-29T10:10:35.717632Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.717621Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.LBFGS([x, y], #Parameters on which the optimization is performed. \n",
    "                              lr = 1, #learning rate, useless when using Wolfe linear search\n",
    "                              max_iter = 200, #maximum number of iterations, but not evaluations !\n",
    "                              line_search_fn = 'strong_wolfe', #linear search algorithm\n",
    "                              tolerance_grad = -1, #stopping criterion for the gradient, deactivated here\n",
    "                              tolerance_change = -1) #stopping criterion for the loss stability, deactivated here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb3689-7821-4e8e-8ebb-2b7fb3734474",
   "metadata": {},
   "source": [
    "And now, the optimization ! First we define a closure() function which will be used to evaluate the function value and to backpropagate the gradients using the .backward() function. Next, just calling the .step(closure) method will apply the closure() function to evaluate the function, calculate its gradient and update the parameters accordingly as many times as needed until convergence criteria (or maximum number of iterations) are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42061c-4547-407a-a726-e7a18cb19cac",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.718133Z",
     "iopub.status.idle": "2024-03-29T10:10:35.718314Z",
     "shell.execute_reply": "2024-03-29T10:10:35.718251Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.718244Z"
    }
   },
   "outputs": [],
   "source": [
    "def closure():\n",
    "    optimizer.zero_grad() #In Pytorch, gradients are accumulated throughout the optimization, so it is important to zero them out before each iteration\n",
    "    loss = f(x, y) #Loss evaluation\n",
    "    loss.backward() #Gradients calculation and propagation\n",
    "    list_xy.append([x.detach().clone().numpy(), y.detach().clone().numpy()]) #Let's store the trajectory !\n",
    "    return loss\n",
    "\n",
    "optimizer.step(closure) #With the .step(closure) function, the gradients calculated by .backward() are applied to each parameters !\n",
    "\n",
    "print(list_xy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629c25b7-fda1-48b1-a995-da458625eac9",
   "metadata": {},
   "source": [
    "Now, let's plot the results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10654fe-e69f-4b5f-bdb8-66e7edd968ea",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.718696Z",
     "iopub.status.idle": "2024-03-29T10:10:35.718875Z",
     "shell.execute_reply": "2024-03-29T10:10:35.718815Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.718809Z"
    }
   },
   "outputs": [],
   "source": [
    "x_grid, y_grid = torch.meshgrid(torch.linspace(-3, 3, 100),\n",
    "                                torch.linspace(-3, 3, 100), \n",
    "                                indexing = 'ij')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d') \n",
    "ax.plot_surface(x_grid, y_grid, f(x_grid, y_grid), cmap = 'viridis', alpha = 0.5)\n",
    "ax.plot(np.asarray(list_xy)[:,0], np.asarray(list_xy)[:,1], \n",
    "           f(np.asarray(list_xy)[:,0], np.asarray(list_xy)[:,1]), \n",
    "            '--o', c = 'orange')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_zlabel('$z$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b36f08-3ff7-4d8a-a452-01ea2907e257",
   "metadata": {},
   "source": [
    "PS : Don't worry about the huge spikes you see, they represent the evaluations of the L-BFGS algorithm ! There is no easy way to select only the iterations of the L-BFGS optimizer in Pytorch, but in the code provided with the PP the evaluations are filtered out and you will see only the iterations !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87815dba-02c5-4405-9011-5f9f8ca178ca",
   "metadata": {},
   "source": [
    "# Creating a Neural Network with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fbf05-169b-4453-af3a-380453bc18d1",
   "metadata": {},
   "source": [
    "In this section we will see how to create a Neural Network with Pytorch ! Don't worry, for the programming practical, the NN is almost already coded so you won't have to worry too much about having to construct it from scratch ! First, let's see how to create a basic NN with Sequential() ! The Linear() layers refers to affine transformation with dimension Linear(input_dim, output_dim) and the activation functions are very simple to introduce !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aad84b-85d1-4af0-8cca-7cbec12ff652",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.719284Z",
     "iopub.status.idle": "2024-03-29T10:10:35.719407Z",
     "shell.execute_reply": "2024-03-29T10:10:35.719350Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.719343Z"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(1, 10),\n",
    "                     nn.Tanh(),\n",
    "                     nn.Linear(10, 30),\n",
    "                     nn.Tanh(),\n",
    "                     nn.Linear(30,1))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de8bcad-c363-47ad-a5ed-34d111150560",
   "metadata": {},
   "source": [
    "Now if you want to define a Neural Network in a more customizable way, you can create a sub-class of the nn.Module class. The forward() method defines how to model acts on an input to create its output when you call model(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b0ba9-727c-44f9-b8b6-e08fc953914c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.719733Z",
     "iopub.status.idle": "2024-03-29T10:10:35.719851Z",
     "shell.execute_reply": "2024-03-29T10:10:35.719795Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.719789Z"
    }
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.layers = layers\n",
    "        self.list_loss = []\n",
    "        self.hidden = nn.ModuleList()\n",
    "        for input_size, output_size in zip(self.layers, self.layers[1:]):\n",
    "            self.hidden.append(nn.Linear(input_size, output_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        L = len(self.hidden)\n",
    "        for (l, linear_transform) in zip(range(L), self.hidden):\n",
    "            if l < L - 1:\n",
    "                input = self.activation(linear_transform(input))\n",
    "            else:\n",
    "                output = linear_transform(input)\n",
    "        return output\n",
    "\n",
    "model = NN([1, 10, 31, 1])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181fcac-fcf5-49cb-ac4d-61f23bf92c7d",
   "metadata": {},
   "source": [
    "# Training a Neural Network with Pytorch (regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0646763-d6a5-49cf-8d44-6489049783e8",
   "metadata": {},
   "source": [
    "For the last section, let's now learn how to train your Neural Network ! For that, you just need to combine all the previous sections and define an optimizer to optimize the Neural Network parameters and minimize a given loss ! Let's see that on a basic 1D regression example. First, let's define the data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "191eaa51-c921-43cc-95e0-f4af33d48e7d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.720344Z",
     "iopub.status.idle": "2024-03-29T10:10:35.720558Z",
     "shell.execute_reply": "2024-03-29T10:10:35.720500Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.720494Z"
    }
   },
   "outputs": [],
   "source": [
    "N_data = 50\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "N_train = int(train_test_ratio*N_data)\n",
    "N_test = int((1-train_test_ratio)*N_data)\n",
    "\n",
    "x = torch.linspace(0, 1, N_data).view(-1, 1)\n",
    "y_exact = x**3 + - torch.exp(x) + 2 #The function we want to approximate\n",
    "y_noisy = y_exact + 0.05 * torch.randn(N_data, 1)  #Let's add some noise\n",
    "\n",
    "indices = torch.randperm(N_data)\n",
    "x_train, y_train = x[indices[:N_train]], y_noisy[indices[:N_train]]\n",
    "x_test, y_test = x[indices[N_train:]], y_noisy[indices[N_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbd30e4-b759-40ce-80de-03bebd0d26fe",
   "metadata": {},
   "source": [
    "Let's visualize our data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60306847-f8df-4258-9c25-5665d9a539f4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.720896Z",
     "iopub.status.idle": "2024-03-29T10:10:35.721114Z",
     "shell.execute_reply": "2024-03-29T10:10:35.721037Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.721026Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.plot(x.detach().clone().cpu().numpy(), \n",
    "         y_exact.detach().clone().cpu().numpy(), label = '$y_{exact}$')\n",
    "plt.plot(x_train.detach().clone().cpu().numpy(), \n",
    "         y_train.detach().clone().cpu().numpy(), 'bo', label = 'training data')\n",
    "plt.plot(x_test.detach().clone().cpu().numpy(), \n",
    "         y_test.detach().clone().cpu().numpy(), 'go', label = 'testing data')\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Noisy data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299308c0-98d3-4a9f-bc02-9c649e37bdae",
   "metadata": {},
   "source": [
    "Now we can define our Neural Network, as well as the optimizer and the loss..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42e6a065-7ff5-42c5-8b34-4ff595dbdaf5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.721537Z",
     "iopub.status.idle": "2024-03-29T10:10:35.721668Z",
     "shell.execute_reply": "2024-03-29T10:10:35.721610Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.721603Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NN([1, 20, 1])\n",
    "\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), #We want to optimize the NN parameters !\n",
    "                              lr = 1, \n",
    "                              max_iter = 200, \n",
    "                              line_search_fn = 'strong_wolfe', \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1) \n",
    "\n",
    "def loss(model, x_train, y_train):\n",
    "    y_pred = model(x_train)\n",
    "    return torch.norm(y_pred - y_train, p=2)**2 #Let's keep the usual 2-norm squared loss.\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    l_train = 1/N_train*loss(model, x_train, y_train)\n",
    "    l_train.backward()\n",
    "    l_test = 1/N_test*loss(model, x_test, y_test)\n",
    "    model.list_loss.append([l_train.detach().clone().cpu().numpy(),\n",
    "                            l_test.detach().clone().cpu().numpy()]) #Let's store the losses value !\n",
    "    if (len(model.list_loss)%50 == 0):\n",
    "        print('iteration {:.0f} ongoing !'.format(len(model.list_loss)))\n",
    "    return l_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2c162-b4f0-4b7d-852d-44df9f2a5314",
   "metadata": {},
   "source": [
    "... and let's go for the training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feebd4fb-58e7-4f7f-8074-25db726a4ed8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.722137Z",
     "iopub.status.idle": "2024-03-29T10:10:35.722278Z",
     "shell.execute_reply": "2024-03-29T10:10:35.722214Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.722207Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d0e85-61ec-4761-a643-20abefac3a09",
   "metadata": {},
   "source": [
    "Now let's plot the results to understand how the training went. Can you see any issue with this setup ? Do you know how you could avoid it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5726171-3f54-4a5d-aa02-8fc66f0ceb1d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-29T10:10:35.722812Z",
     "iopub.status.idle": "2024-03-29T10:10:35.722999Z",
     "shell.execute_reply": "2024-03-29T10:10:35.722938Z",
     "shell.execute_reply.started": "2024-03-29T10:10:35.722931Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x_train.detach().clone().cpu().numpy(), \n",
    "         y_train.detach().clone().cpu().numpy(), 'bo', label = 'training data')\n",
    "plt.plot(x_test.detach().clone().cpu().numpy(), \n",
    "         y_test.detach().clone().cpu().numpy(), 'go', label = 'testing data')\n",
    "plt.plot(x.detach().clone().cpu().numpy(), \n",
    "         y_exact.detach().clone().cpu().numpy(), label = '$y_{exact}$')\n",
    "plt.plot(x.detach().clone().cpu().numpy(), \n",
    "         y_pred.detach().clone().cpu().numpy(), label = '$y_{pred}$')\n",
    "plt.legend(loc = 'best')\n",
    "plt.title('Model after training')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "array_loss = np.asarray(model.list_loss)\n",
    "plt.plot(array_loss[:, 0], label = 'train')\n",
    "plt.plot(array_loss[:, 1], label = 'test')\n",
    "plt.title('Loss function convergence')\n",
    "plt.xlabel('L-BFGS iterations + evaluations')\n",
    "plt.ylabel('Loss value')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479c253-e57e-456d-b785-e3fe075c1ce5",
   "metadata": {},
   "source": [
    "PS : Same thing as before for the spikes in the loss, they represent evaluations ! You won't have to worry about them in the code provided for the PP. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP- Recalage d'images\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problème fréquemment rencontré dans le domaine du traitement d’images est celui du recalage. On dispose de plusieurs images prises à des temps différents, ou par des appareils différents, et on aimerait les mettre en correspondence, c’est-à-dire trouver une déformation du plan, qui assure une correspondence point à point des objets sous-jacents. Donnons quelques exemples d’applications :\n",
    "* Traitements/retouches d’images. Par exemple, on peut vouloir construire un panoramique à partir d’images de petite taille. Il faut les recaler préalablement.\n",
    "* Evaluation des déplacements d’objets dans des séquences vidéos (e.g. trouver un défaut de fonctionnement d’un organe, caméras de surveillance, design de robots intelligents ou de systèmes de navigation automatiques ...)\n",
    "* Couplage d’informations. Par exemple, en imagerie médicale, on obtient une information plus riche en utilisant à la fois une radio et une angiographie. L’une apporte des informations structurelles, l’autre des informations fonctionnelles. Le couplage des deux images donne plus d’information au praticien.\n",
    "* Beaucoup d’autres applications...\n",
    "\n",
    "Dans ce TP, nous allons proposer un modèle de recalage assez élémentaire. Les idées constitutives se retrouvent cependant dans presque toutes les techniques récentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images() :\n",
    "    n=21\n",
    "    sigma=0.3\n",
    "    [X,Y]=np.meshgrid(np.linspace(-1,1,n),np.linspace(-1,1,n), indexing='xy')\n",
    "    Z=np.sqrt(X*X+Y*Y)\n",
    "    im1=np.zeros((n,n))\n",
    "    im1[Z<=.7]=1.\n",
    "    im1[Z<=.3]=.5\n",
    "    im1[Z<=.1]=.7\n",
    "    im2=np.zeros((n,n));\n",
    "    Z=np.sqrt((X-.3)**2+(Y+.2)**2)\n",
    "    im2[Z<=.7]=1\n",
    "    im2[Z<=.3]=.5\n",
    "    im2[Z<=.1]=.7\n",
    "    G=np.fft.fftshift(np.exp(-(X**2+Y**2)/sigma**2))\n",
    "    f=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im1)))\n",
    "    g=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im2))) \n",
    "    f=f/np.max(f)\n",
    "    g=g/np.max(g)\n",
    "    return f,g,im1,im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,g,im1,im2=get_images()\n",
    "fig, axarr = plt.subplots(2, 2, figsize=(10,10))\n",
    "axarr[0,0].imshow(im1)\n",
    "axarr[0,0].set_title('Image im1')\n",
    "axarr[1,0].imshow(f)\n",
    "axarr[1,0].set_title('function f ( blurry version of im1)')\n",
    "axarr[0,1].imshow(im2)\n",
    "axarr[0,1].set_title('Image im2')\n",
    "axarr[1,1].imshow(g)\n",
    "axarr[1,1].set_title('function g ( blurry version of im2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Formalisation du problème\n",
    "## 1.1 Formalisme continu\n",
    "\n",
    "On modélise les images en niveaux de gris comme des fonctions d’un ensemble borné $\\Omega\\subset \\mathbb{R}$ (typiquement un carré) dans $\\mathbb{R}$. La valeur de la fonction en chaque point représente l’intensité lumineuse de l’image. \n",
    "\n",
    "Soient $f$ et $g$ deux images. On a donc :\n",
    "$$\n",
    "f:\\Omega\\subset \\mathbb{R}^2 \\to \\mathbb{R},  g:\\Omega\\subset \\mathbb{R}^2 \\to \\mathbb{R} \n",
    "$$\n",
    "En supposant que les images $f$ et $g$ dépendent seulement d’une transformation géométrique qui conserve la luminosité, le problème de recalage peut être formulé comme suit:\n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 255, 0.1); padding:15px;margin:5px\">\n",
    "Problème $(P_1)$ : <br>\n",
    "Etant donnés $f$ et $g$ dans $H_1(\\Omega)$ (les images ont une amplitude bornée et une énergie finie), trouver un champ de vecteurs $u = (u_1, u_2) \\in H_1(\\Omega)^2$ tel que:\n",
    "$$f(x + u(x)) = g(x), \\forall x\\in \\Omega.$$\n",
    "</div>\n",
    "\n",
    "Le problème inverse est mal posé: tout d'abord, l'existence d'une solution n'est pas garantie, et dans le cas où il existe une solution, on n'a pas nécessairement unicité de cette solution. Par exemple, si $f$ et $g$ sont des fonctions constantes, n'importe quel déplacement $u$ est solution. Si $f$ est la constante $1$ et si $g$ est la constante $0$, il n'existe pas de solutions.\n",
    "\n",
    "Pour le résoudre, on se propose de le reformuler comme un problème d'optimisation: \n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 255, 0.1); padding:15px;margin:5px\">\n",
    "Problème $(P_2)$ : <br>\n",
    "On cherche une déformation $u$ du plan qui minimise:\n",
    "$$\n",
    "E(u)=\\displaystyle\\frac{1}{2}\\int_\\Omega (f(x+u(x))-g(x))^2 dx=\\frac{1}{2}\\|f\\circ (id+u)-g\\|^2.\n",
    "$$\n",
    "</div>\n",
    "\n",
    "Sans hypothèse supplémentaire, le problème $\\displaystyle \\min_{u \\in H^1(\\Omega)^2} E(u)$ n'est a priori pas convexe, toujours mal posé et même éventuellement non différentiable si $u$ et $f$ ne sont pas assez régulières. On pourrait facilement rendre $f$ différentiable (en ajoutant du flou (on dit aussi du \"bruit\") par exemple à l'image, ce qui revient à convoler $f$ avec une gaussienne) mais il faut également \"forcer\" $u$ à être différentiable. Pour cela on propose de régulariser le problème de façon à assurer la convexité du problème d'optimisation considéré ainsi que l'existence et l'unicité des solutions.\n",
    "\n",
    "Pour régulariser le problème inverse, nous allons faire une analogie avec l’élasticité linéaire. La fonction $u = (u_x,u_y)$ représente un champ de déformations. En notant $\\partial x$ et $\\partial y$ les opérateurs de dérivation partielle par rapport à chacun des axes du plan, on peut définir un potentiel élastique linéarisé :\n",
    "$$\n",
    "R(u)= \\frac{\\mu}{2}\\int_{\\Omega} \\underbrace{ (\\partial_x u_y + \\partial_y u_x)^2(x,y) dxdy}_{R_1(u)=\\textrm{cisaillement}} +\\frac{\\lambda+\\mu}{2}\\int_{\\Omega} \\underbrace{(\\partial_x u_x + \\partial_y u_y)^2(x,y) dxdy}_{R_2(u)=\\textrm{variations  de  volume}}.\n",
    "$$ \n",
    "En mécanique des structures, $\\mu$ et $\\lambda$ sont appelées constantes de Lamé. Le paramètre $\\lambda$ n’a pas d’interprétation directe, tandis que le paramètre $\\mu$ est appelé module de cisaillement.\n",
    "\n",
    "Le problème d'optimisation à résoudre dans ce TP est le suivant:\n",
    "<div style=\"background-color:rgba(0, 0, 255, 0.1); padding:15px;margin:5px\">\n",
    "$$(P)\\qquad \\min_{u} E(u)+R(u).$$\n",
    "$$\n",
    "E(u)=\\displaystyle\\frac{1}{2}\\int_\\Omega (f(x+u(x))-g(x))^2 dx=\\frac{1}{2}\\|f\\circ (id+u)-g\\|^2.\n",
    "$$\n",
    "$$\n",
    "R(u)= \\frac{\\mu}{2}\\int_{\\Omega} \\underbrace{ (\\partial_x u_y + \\partial_y u_x)^2(x,y) dxdy}_{R_1(u)=\\textrm{cisaillement}} +\\frac{\\lambda+\\mu}{2}\\int_{\\Omega} \\underbrace{(\\partial_x u_x + \\partial_y u_y)^2(x,y) dxdy}_{R_2(u)=\\textrm{variations  de  volume}}.\n",
    "$$    \n",
    "</div>\n",
    "\n",
    "**Q1.** A l'aide d'un développement de Taylor, vérifier que le gradient de $E$ s'écrit:\n",
    "\n",
    "$$\\nabla E(u) = \\left(f\\circ (id+u) -g\\right)\\nabla f\\circ (id+u)$$\n",
    "\n",
    "au sens où la différentielle de $E$ est définie par:\n",
    "\n",
    "$$dE(u)\\cdot h= \\displaystyle\\int_\\Omega \\langle (f(x+u(x))-g(x))\\nabla f(x+u(x)),h(x)\\rangle dx.$$\n",
    "\n",
    "Puis par définition, on a $$ dE(u)\\cdot h =\\langle \\nabla E(u),h\\rangle \\quad \\forall h$$\n",
    "<div style=\"background-color:rgba(255, 0,0, 0.1); padding:15px;\">\n",
    "Votre réponse ici\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Discrétisation\n",
    "\n",
    "Pour pouvoir résoudre numériquement le problème $(P)$ (dont les variables de l'optimisation sont des fonctions !), on propose de le discrétiser au préalable. \n",
    "\n",
    "Soit $1\\le i \\le n$ and $1\\le j\\le m$. Notons $(x_i,y_j)$ le point de la grille $(i,j)$ et $f_{i,j}$ la valeur de $f$ au point $(x_i,y_j)$. Le produit scalaire sur $V=\\mathcal M_{n,m}(\\mathbb{R})$ est défini par:\n",
    "$$\\langle f,g\\rangle_V=\\sum_{i=1}^n\\sum_{j=1}^m f_{i,j}g_{i,j},$$\n",
    "\n",
    "## 2.1. Calcul du $E$ et de son gradient\n",
    "Pour pouvoir calculer $E$ et son gradient, on va avoir besoin d'évaluer $f\\circ (Id+u)$ et $\\nabla f\\circ(id+u)$. Pour cela, vous aurez besoin de la fonction `interpol` ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpol(function,ux,uy) :\n",
    "    # function that computes f \\circ Id+u and interpolates it on a mesh\n",
    "    nx,ny=f.shape\n",
    "    ip=interpolate.RectBivariateSpline(np.arange(nx),np.arange(ny),function)\n",
    "    [X,Y]=np.meshgrid(np.arange(nx),np.arange(ny), indexing='ij')\n",
    "    X=X+ux\n",
    "    Y=Y+uy\n",
    "    return np.reshape(ip.ev(X.ravel(),Y.ravel()),(nx,ny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Lisez la documentation de la fonction `RectBivariateSpline` afin de comprendre ce que fait la fonction `interpol`.\n",
    "\n",
    "<div style=\"background-color:rgba(255, 0,0, 0.1); padding:15px;\">\n",
    "Votre réponse ici\n",
    "</div>\n",
    "\n",
    "\n",
    "## 2.2. Calcul de $R$ et de son gradient\n",
    "On discrétise également les opérateurs de dérivation partielles par différences finies ; par exemple la dérivée partielle par rapport à $x$ est donnée par, pour tout $f\\in V$ par\n",
    "$$\\begin{cases}(\\partial_x f)_{i,j}=f_{i+1,j}-f_{i,j} \\text{ si } i<n \\\\\n",
    "(\\partial_x f)_{n,j}=0 \\end{cases}.$$\n",
    "On a ainsi $\\partial_x f\\in V$.\n",
    "\n",
    "On définit les opérateurs $\\partial_x^T$ et $\\partial_y^T$ comme les uniques opérateurs qui vérifient\n",
    "\n",
    "$$ \\langle \\partial_x^T f, g\\rangle_V = \\langle f,\\partial_x  g\\rangle_V \\quad \\forall f,g\\in V$$\n",
    "$$ \\langle \\partial_y^T f, g\\rangle_V = \\langle f,\\partial_y  g\\rangle_V \\quad \\forall f,g\\in V$$\n",
    "\n",
    "**Q2.** Donner les formules de discrétisation des opérateurs $\\partial_y$, $\\partial_x^\\top$ et $\\partial_y^\\top$. Implémenter ces opérateurs ci-après. On donne un test de vérification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    d[:-1,:]=im[1:,:]-im[:-1,:]\n",
    "    return d\n",
    "def dy(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    # To be implemented\n",
    "    return d\n",
    "def dyT(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    # To be implemented\n",
    "    return d  \n",
    "def dxT(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    # To be implemented\n",
    "    return d\n",
    "\n",
    "def scal(a,b) :\n",
    "    return np.sum(a*b)\n",
    "\n",
    "np.random.seed(42)\n",
    "im1=np.random.randn(12,15)\n",
    "im2=np.random.randn(12,15)\n",
    "print(scal(dx(im1),im2),'=',scal(im1,dxT(im2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.**  On peut alors écrire :\n",
    "$$\n",
    "R(u)= \\frac{\\mu}{2}\\underbrace{\\sum_{i}(\\partial_x u_y + \\partial_y u_x)^2(i)}_{=R_1(u)} + \\frac{\\lambda+\\mu}{2} \\underbrace{\\sum_i(\\partial_x u_x + \\partial_y u_y)^2(i)}_{=R_2(u)}.\n",
    "$$ \n",
    "où:\n",
    "* $u_x\\in V$ et $u_y\\in V$ sont les discrétisations des composantes du champ de vecteurs $u$ sur la grille choisie\n",
    "* $\\partial_x:V \\rightarrow V$ et $\\partial_y:V\\rightarrow V$ représentent les opérateurs de différences finies.\n",
    "\n",
    "On peut ré-écrire $R(u)=\\frac{\\mu}{2}R_1(u)+ \\frac{\\lambda+\\mu}{2} R_2(u)$ avec : \n",
    "$$\n",
    "R_1(u)=\\langle A_1 u , A_1 u\\rangle_V,\\qquad R_2(u)=\\langle A_2 u , A_2 u\\rangle_V.\n",
    "$$\n",
    "Où $A_i$ est un opérateur de $V^2$ dans $V$.\n",
    "Donnez l'expression des matrices $A_1$ et $A_2$ en fonction des opérateurs $\\partial_x$ et $\\partial_y$.\n",
    "\n",
    "\n",
    "<div style=\"background-color:rgba(255, 0,0, 0.1); padding:15px;\">\n",
    "Votre réponse ici\n",
    "</div>\n",
    "\n",
    "\n",
    "**Q3b.**\n",
    "Pour tout $u=(u_x,u_y)$ et $v=(v_x,v_y)$ dans $V^2$, on définit le produit scalaire dans $V^2$ par\n",
    "$$\\langle u,v\\rangle_{V^2}=\\langle u_x,v_x\\rangle_{V}+\\langle u_y,v_y\\rangle_{V}$$\n",
    "\n",
    "Montrer que $R(u)$ peut s'écrire sous la forme:\n",
    "$$R(u)=\\frac{1}{2} \\langle A\\left(\\begin{array}{c}\n",
    "u_x\\\\\n",
    "u_y\n",
    "\\end{array}\\right),\\left(\\begin{array}{c}\n",
    "u_x\\\\\n",
    "u_y\n",
    "\\end{array}\\right)\\rangle_{V^2},$$\n",
    "\n",
    "Avec $$A=\\mu A_1^TA_1 +(\\lambda+\\mu) A_2^TA_2$$ et $A_i^T$ un opérateur de $V^2$ dans $V$.\n",
    "\n",
    "<div style=\"background-color:rgba(255, 0,0, 0.1); padding:15px;\">\n",
    "Votre réponse ici\n",
    "</div>\n",
    "\n",
    "**Q4.** Donner l'expression du gradient de $R$.\n",
    "<div style=\"background-color:rgba(255, 0,0, 0.1); padding:15px;\">\n",
    "Votre réponse ici\n",
    "</div>\n",
    "\n",
    "## 2.3. Implémentation de la fonction objectif $E+R$\n",
    "\n",
    "Créez une fonction `objective_function` qui calcul $E(u)+R(u)$. \n",
    "\n",
    "Cette fonction prend en variable d'entrée \n",
    "* `f`,`g` et `ux` et `uy` qui sont nécessaires au calcul de $E(u)$\n",
    "* `ux`,`uy` ,`lamb` et `mu` qui sont nécessaires au calcul de $R(u)$. Ici `lamb` et `mu` sont deux variables réelles positives qui représentent $\\lambda$ et $\\mu$. \n",
    "\n",
    "\n",
    "Cette fonction doit rendre deux arguments :\n",
    "* `obj` qui est la valeur de $E(u)+R(u)$\n",
    "* `fu` qui est un tableau qui représente le résultat du calcul de $f\\circ(Id +u)$. On rend ce tableau car il sera utile pour les prochains calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(f,g,ux,uy,lamb,mu) :\n",
    "    raise ValueError('to be implemented') \n",
    "    return obj,fu\n",
    "\n",
    "np.random.seed(12)\n",
    "ux=np.random.randn(f.shape[0],f.shape[1])\n",
    "uy=np.random.randn(f.shape[0],f.shape[1])\n",
    "obj,fu=objective_function(2*f,3*g,ux,uy,10,5)\n",
    "print(obj,fu.shape,np.linalg.norm(fu)) #18215.406074781582 (21, 21) 22.047265562734097"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Implémentation des gradients\n",
    "Créez une fonction `gradE` et `gradR` qui calcule $\\nabla E(u)$ et $\\nabla R(u)$. On garde les conventions de noms de variable de la section précédente et on notera `gradxE,gradyE` les deux composantes de  $\\nabla E(u)$ et `gradxR,gradyR` les deux composantes de $\\nabla R(u)$. Les variables `dfx` et `dfy`contiennent les dérivées selon `x` et `y` de `f`, elles seront initalisées par `dfx=dx(f)` et `dfy=dy(f)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradE(dfx,dfy,g,fu,ux,uy) :\n",
    "    return gradxE,gradyE\n",
    "\n",
    "def gradR(lamb,mu,ux,uy) :\n",
    "    return gradxR,gradyR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx=dx(f)\n",
    "dfy=dy(f)  \n",
    "np.random.seed(12)\n",
    "ux=np.random.randn(f.shape[0],f.shape[1])\n",
    "uy=np.random.randn(f.shape[0],f.shape[1])\n",
    "obj,fu=objective_function(f,g,ux,uy,10,5)\n",
    "gradxE,gradyE=gradE(dfx,dfy,g,fu,ux,uy)\n",
    "print(gradxE.shape,gradyE.shape) #(21, 21) (21, 21)\n",
    "print(np.linalg.norm(gradxE),np.linalg.norm(gradyE)) #0.6513993385167333 0.6040320505869908\n",
    "gradxR,gradyR=gradR(10.,5.,ux,uy) \n",
    "print(gradxR.shape,gradyR.shape)  #(21, 21) (21, 21)\n",
    "print(np.linalg.norm(gradxR),np.linalg.norm(gradyR)) #1180.7792368512296 1239.1880693357143"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Un algorithme de gradient\n",
    "\n",
    "Une itération de la méthode de descente de gradient est de la forme:\n",
    "\n",
    "$$\n",
    "u_{k+1}=u_k-s_k(\\nabla E(u) + \\nabla R(u))\n",
    "$$\n",
    "\n",
    "**Q5.** Compléter la fonction RecalageDG implémentant la descente de gradient et utilisant l'algorithme de recherche linéaire par rebroussement proposé ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(ux,uy,step,descentx,descenty,obj_old,f,g,lamb,mu) :\n",
    "    step=2*step\n",
    "    tmpx=ux-step*descentx\n",
    "    tmpy=uy-step*descenty\n",
    "    obj,fu=objective_function(f,g,tmpx,tmpy,lamb,mu)\n",
    "    while obj >obj_old and step > 1.e-8:\n",
    "        step=0.5*step\n",
    "        tmpx=ux-step*descentx\n",
    "        tmpy=uy-step*descenty\n",
    "        obj,fu=objective_function(f,g,tmpx,tmpy,lamb,mu)\n",
    "    return tmpx,tmpy,step\n",
    "\n",
    "\n",
    "def RecalageDG(f,g,lamb,mu,nitermax,stepini) : \n",
    "    ux=np.zeros(f.shape)\n",
    "    uy=np.zeros(f.shape)  \n",
    "    CF=[]\n",
    "    step_list=[]\n",
    "    niter=0\n",
    "    step=stepini\n",
    "    raise ValueError('Compute dfx and dfy here')\n",
    "    while niter < nitermax and step > 1.e-8 : \n",
    "        niter+=1\n",
    "        obj,fu=objective_function(f,g,ux,uy,lamb,mu)\n",
    "        CF.append(obj)   \n",
    "        raise ValueError('Compute gradxE and gradyE here')\n",
    "        raise ValueError('Compute gradRx and gradRy here')\n",
    "        raise ValueError('Compute gradx and grady here')\n",
    "        ux,uy,step=linesearch(ux,uy,step,gradx,grady,obj,f,g,lamb,mu)\n",
    "        step_list.append(step)\n",
    "        if (niter % 3 ==0) :\n",
    "            print('iteration :',niter,' cost function :',obj,'step :',step)\n",
    "    return ux,uy,np.array(CF),np.array(step_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Ecrire un compte-rendu des expériences réalisées et des résultats obtenus. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb=10\n",
    "mu=20\n",
    "nitermax=500\n",
    "\n",
    "step0 = 0.01\n",
    "ux,uy,CF,step=RecalageDG(f,g,lamb,mu,nitermax,step0)\n",
    "# la premiere ligne est \n",
    "# iteration : 3  cost function : 19.08125492560699 step : 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(10,10))\n",
    "ax[0,0].imshow(f)\n",
    "ax[0,0].set_title('original function')\n",
    "ax[0,1].imshow(g)\n",
    "ax[0,1].set_title('target function')\n",
    "ax[1,0].quiver(ux,uy)\n",
    "ax[1,0].set_title('displacement field')\n",
    "ax[1,1].imshow(interpol(f,ux,uy))\n",
    "ax[1,1].set_title('final function')\n",
    "ax[0,2].plot(CF)\n",
    "ax[0,2].set_title('objective history')\n",
    "ax[1,2].plot(np.log(step))\n",
    "ax[1,2].set_title('step history (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algorithme de moindres carrés.\n",
    "\n",
    "On souhaite maintenant implémenter un algorithme de second ordre pour résoudre le problème $$(P)\\quad\\min_u E(u)+R(u)$$ afin d'accélérer la convergence de l'algorithme. Pour cela, on va reformuler le problème $(P)$ en un problème de moindres carrés et appliquer l'algorithme de Levenberg-Marquardt.\n",
    "\n",
    "Soit:\n",
    "$$\\Psi(u)=\\begin{pmatrix} \n",
    "f\\circ(Id+u)-g \\\\ \n",
    "\\sqrt{\\mu}(\\partial_xu_y+\\partial_yu_x) \\\\ \n",
    "\\sqrt{\\mu+\\lambda}(\\partial_xu_x+\\partial_yu_y) \\end{pmatrix},$$\n",
    "où $f\\circ(id+u)$ est l'interpolation de $x\\mapsto f(x+u(x))$ sur la grille. Minimiser $E(u)+R(u)$ est équivalent à résoudre le problème suivant:\n",
    "\n",
    ">$$\\min_u \\|\\Psi(u)\\|_2^2.$$\n",
    "\n",
    "Il s'agit maintenant d'un problème de moindres carrés que l'on va résoudre à l'aide de l'algorithme de Levenberg Marquardt :\n",
    "\n",
    "$$\n",
    "u_{k+1}=u_k- H_k^{-1} J_{\\Psi}(u_k)^\\top \\Psi(u_k) \\quad\\mbox{ avec }\\quad H_k=J_{\\Psi}(u_k)^\\top J_{\\Psi}(u_k) +\\varepsilon Id\n",
    "$$\n",
    "\n",
    "**Q6.** Calculer la matrice jacobienne de $\\Psi$, notée $J_\\Psi(u)$.\n",
    "<div style=\"background-color:rgba(255, 0,0, 0.1); padding:15px;\">\n",
    "Votre réponse ici\n",
    "</div>\n",
    "\n",
    "**Q7.** Implémenter les fonctions `Psi`,`JPsi`, `JTPsi` et `JTJ` qui calculent respectivement:\n",
    "- La valeur de $\\Psi(u)$\n",
    "- le produit de $J_\\Psi(u)$ par une direction $v=(v_x,v_y)\\in V^2$,\n",
    "\n",
    "- le produit de $J_\\Psi(u)^\\top$ par $\\phi=(\\phi_1,\\phi_2,\\phi_3)\\in V^3$,\n",
    "\n",
    "- le produit de $(J_\\Psi(u)^\\top J_\\Psi(u)+\\epsilon I)$ par une direction $v=(v_x,v_y)\\in V^2$.\n",
    "\n",
    "\n",
    "Par convention `fu` représente $f\\circ(Id+u)$ et `dfxu`et `dfyu` représentent les deux coordonnées de $(\\nabla f)\\circ(Id +u)$. On vous donne dans la cellulle suivante des tests de vérification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Psi(fu,g,ux,uy,lamb,mu) :\n",
    "    return Psi0,Psi1,Psi2\n",
    "\n",
    "def JPsi(vx,vy,dfxu,dfyu,lamb,mu) :\n",
    "    return JPsi0,JPsi1,JPsi2\n",
    "\n",
    "def JTPsi(phi,dfxu,dfyu,lamb,mu) :\n",
    "    return ux,uy\n",
    "  \n",
    "def JTJ(vx,vy,dfxu,dfyu,lamb,mu,epsilon) :\n",
    "    return uxs,uys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=50\n",
    "np.random.seed(42)\n",
    "fu=np.random.randn(n,n)\n",
    "tmp_g=np.random.randn(n,n)\n",
    "ux=np.random.randn(n,n)\n",
    "uy=np.random.randn(n,n)\n",
    "dfxu=np.random.randn(n,n)\n",
    "dfyu=np.random.randn(n,n)\n",
    "print('## TESTING PSI')\n",
    "Psi0,Psi1,Psi2=Psi(fu,tmp_g,ux,uy,5,10)\n",
    "print(Psi0.shape,Psi1.shape,Psi2.shape) #(50, 50) (50, 50) (50, 50)\n",
    "print(np.linalg.norm(Psi0),np.linalg.norm(Psi1),np.linalg.norm(Psi2)) # 70.85046394690714 320.74675708932546 389.63146864883447\n",
    "print('## TESTING JPSI')\n",
    "Psi0,Psi1,Psi2=JPsi(ux,uy,dfxu,dfyu,5,10)\n",
    "print(Psi0.shape,Psi1.shape,Psi2.shape) # (50, 50) (50, 50) (50, 50)\n",
    "print(np.linalg.norm(Psi0),np.linalg.norm(Psi1),np.linalg.norm(Psi2)) #69.27576209907774 320.74675708932546 389.63146864883447\n",
    "print('## TESTING JTPSI')\n",
    "Psi0=np.random.randn(n,n)\n",
    "Psi1=np.random.randn(n,n)\n",
    "Psi2=np.random.randn(n,n)\n",
    "vx,vy=JTPsi((Psi0,Psi1,Psi2),dfxu,dfyu,5,10)\n",
    "print(vx.shape,vy.shape) # (50, 50) (50, 50)\n",
    "print(np.linalg.norm(vx),np.linalg.norm(vy)) #355.08391793335545 352.46941362403214\n",
    "print('## TESTING JTJ')\n",
    "vx,vy=JTJ(ux,uy,dfxu,dfyu,5.,10.,1.)\n",
    "print(vx.shape,vy.shape) # (50, 50) (50, 50)\n",
    "print(np.linalg.norm(vx),np.linalg.norm(vy)) #3571.889912528785 3522.237528300008\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons maintenant la direction de recherche $d_k$ comme solution du système linéaire:\n",
    "$$(J_\\Psi(u_k)^\\top J_\\Psi(u_k) +\\epsilon I)\\left(\\begin{array}{c}\n",
    "d_x\\\\\n",
    "d_y\n",
    "\\end{array}\\right) = -J_\\Psi(u_k)^\\top \\Psi(u_k)$$\n",
    "Pour cela, on vous donne l'algorithme suivant qui par la méthode du gradient conjugué calcule une solution $d=(d_x,d_y)\\in V^2$ du problème:\n",
    "$$(J_\\Psi(u_k)^\\top J_\\Psi(u_k) +\\epsilon I)\\left(\\begin{array}{c}\n",
    "d_x\\\\\n",
    "d_y\n",
    "\\end{array}\\right) = b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def CGSolve(u0x,u0y,lamb,mu,b,epsilon,dfxu,dfyu) :\n",
    "    nitmax=100\n",
    "    ux,uy=u0x,u0y #point de départ de l'algorithme\n",
    "    # Computes JTJu\n",
    "    Ax,Ay=JTJ(ux,uy,dfxu,dfyu,lamb,mu,epsilon)\n",
    "    rx=b[0]-Ax\n",
    "    ry=b[1]-Ay\n",
    "    px,py=np.copy(rx),np.copy(ry)\n",
    "    rsold=np.linalg.norm(rx)**2+np.linalg.norm(ry)**2\n",
    "    for i in range(nitmax) :\n",
    "        Apx,Apy=JTJ(px,py,dfxu,dfyu,lamb,mu,epsilon);\n",
    "        alpha=rsold/(np.vdot(rx[:],Apx[:])+np.vdot(ry[:],Apy[:]))\n",
    "        ux=ux+alpha*px\n",
    "        uy=uy+alpha*py\n",
    "        rx=rx-alpha*Apx\n",
    "        ry=ry-alpha*Apy\n",
    "        rsnew=np.linalg.norm(rx)**2+np.linalg.norm(ry)**2\n",
    "        if np.sqrt(rsnew)<1e-10 :\n",
    "            return [ux,uy]\n",
    "        px=rx+rsnew/rsold*px\n",
    "        py=ry+rsnew/rsold*py\n",
    "        rsold=rsnew\n",
    "    return ux,uy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** Compléter l'algorithme RecalageGN implémentant la méthode de Levenberg-Marquardt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecalageGN(f,g,lamb,mu,nitermax,stepini,epsi) : \n",
    "    ux=np.zeros(f.shape)\n",
    "    uy=np.zeros(f.shape)  \n",
    "    descentx=np.zeros(f.shape)\n",
    "    descenty=np.zeros(f.shape) \n",
    "    raise ValueError('Compute dfx and dfy here')\n",
    "    CF=[]\n",
    "    step_list=[]\n",
    "    niter=0\n",
    "    step=stepini\n",
    "    while niter < nitermax and step > 1.e-8 : \n",
    "        niter+=1\n",
    "        obj,fu=objective_function(f,g,ux,uy,lamb,mu)\n",
    "        CF.append(obj)\n",
    "        raise ValueError('Compute dfxu,dfyu here')\n",
    "        raise ValueError('Compute b here')\n",
    "        b=JTPsi(Psi(fu,g,ux,uy,mu,lamb),dfxu,dfyu,lamb,mu)\n",
    "        [descentx,descenty]=CGSolve(descentx,descenty,lamb,mu,b,epsi,dfxu,dfyu)\n",
    "        ux,uy,step=linesearch(ux,uy,step,descentx,descenty,obj,f,g,lamb,mu)\n",
    "        step_list.append(step)\n",
    "        # Display\n",
    "        if (niter % 3 ==0) :\n",
    "            print('iteration :',niter,' cost function :',obj,'step :',step)\n",
    "    return ux,uy,np.array(CF),np.array(step_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** Tester le nouvel algorithme et comparer sa vitesse de convergence avec celle de l'algorithme de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsi=0.1\n",
    "nitermax=1000\n",
    "ux,uy,CF,step=RecalageGN(f,g,lamb,mu,nitermax,step0,epsi)\n",
    "# On doit trouver comme première ligne\n",
    "#iteration : 3  cost function : 18.9861786936971 step : 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "ax[0,0].imshow(f)\n",
    "ax[0,0].set_title('original function')\n",
    "ax[0,1].imshow(g)\n",
    "ax[0,1].set_title('target function')\n",
    "ax[1,0].quiver(ux,uy)\n",
    "ax[1,0].set_title('displacement field')\n",
    "ax[1,1].imshow(interpol(f,ux,uy))\n",
    "ax[1,1].set_title('final function')\n",
    "ax[0,2].plot(CF)\n",
    "ax[0,2].set_title('objective history')\n",
    "ax[1,2].plot(np.log(step))\n",
    "ax[1,2].set_title('step history (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Jeu des différences\n",
    "Maintenant que vous avez implémenté et testé les deux algorithmes sur l'image-jouet proposée, voyons que cela donne sur une image IRM d'un cerveau. Saurez-vous détecter les différences/déplacements entre les deux images ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1=Image.open('IRM1.png')\n",
    "im2=Image.open(\"IRM2.png\")\n",
    "plt.imshow(plt.imread('IRM1.png'))\n",
    "plt.show()\n",
    "plt.imshow(plt.imread('IRM2.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n,m]=im1.size\n",
    "sigma=0.1\n",
    "[X,Y]=np.meshgrid(np.linspace(-1,1,n),np.linspace(-1,1,m), indexing='xy')\n",
    "Z=np.sqrt(X*X+Y*Y)\n",
    "G=np.fft.fftshift(np.exp(-(X**2+Y**2)/sigma**2))\n",
    "f=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im1)))\n",
    "g=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im2))) \n",
    "f=f/np.max(f)\n",
    "g=g/np.max(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP1 - Recalage d'images\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy import interpolate\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un problème fréquemment rencontré dans le domaine du traitement d’images est celui du recalage. On dispose de plusieurs images prises à des temps différents, ou par des appareils différents, et on aimerait les mettre en correspondence, c’est-à-dire trouver une déformation du plan, qui assure une correspondence point à point des objets sous-jacents. Donnons quelques exemples d’applications :\n",
    "* Traitements/retouches d’images. Par exemple, on peut vouloir construire un panoramique à partir d’images de petite taille. Il faut les recaler préalablement.\n",
    "* Evaluation des déplacements d’objets dans des séquences vidéos (e.g. trouver un défaut de fonctionnement d’un organe, caméras de surveillance, design de robots intelligents ou de systèmes de navigation automatiques ...)\n",
    "* Couplage d’informations. Par exemple, en imagerie médicale, on obtient une information plus riche en utilisant à la fois une radio et une angiographie. L’une apporte des informations structurelles, l’autre des informations fonctionnelles. Le couplage des deux images donne plus d’information au praticien.\n",
    "* Beaucoup d’autres applications...\n",
    "\n",
    "Dans ce TP, nous allons proposer un modèle de recalage assez élémentaire. Les idées constitutives se retrouvent cependant dans presque toutes les techniques récentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images() :\n",
    "    n=21\n",
    "    sigma=0.3\n",
    "    [X,Y]=np.meshgrid(np.linspace(-1,1,n),np.linspace(-1,1,n), indexing='xy')\n",
    "    Z=np.sqrt(X*X+Y*Y)\n",
    "    im1=np.zeros((n,n))\n",
    "    im1[Z<=.7]=1.\n",
    "    im1[Z<=.3]=.5\n",
    "    im1[Z<=.1]=.7\n",
    "    im2=np.zeros((n,n));\n",
    "    Z=np.sqrt((X-.3)**2+(Y+.2)**2)\n",
    "    im2[Z<=.7]=1\n",
    "    im2[Z<=.3]=.5\n",
    "    im2[Z<=.1]=.7\n",
    "    G=np.fft.fftshift(np.exp(-(X**2+Y**2)/sigma**2))\n",
    "    f=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im1)))\n",
    "    g=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im2))) \n",
    "    f=f/np.max(f)\n",
    "    g=g/np.max(g)\n",
    "    return f,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,g=get_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Formalisation du problème"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Formalisme continu\n",
    "---------------------\n",
    "\n",
    "On modélise les images en niveaux de gris comme des fonctions d’un ensemble borné $\\Omega\\subset \\mathbb{R}$ (typiquement un carré) dans $\\mathbb{R}$. La valeur de la fonction en chaque point représente l’intensité lumineuse de l’image. \n",
    "\n",
    "Soient $f$ et $g$ deux images. On a donc :\n",
    "$$\n",
    "f:\\Omega\\subset \\mathbb{R}^2 \\to \\mathbb{R},  g:\\Omega\\subset \\mathbb{R}^2 \\to \\mathbb{R} \n",
    "$$\n",
    "En supposant que les images $f$ et $g$ dépendent seulement d’une transformation géométrique qui conserve la luminosité, le problème de recalage peut être formulé comme suit:\n",
    "\n",
    "> Problème inverse $(P_1)$ : \n",
    "\n",
    "> Etant donnés $f$ et $g$ dans $H_1(\\Omega)$ (les images ont une amplitude bornée et une énergie finie), trouver un champ de vecteurs $u = (u_1, u_2) \\in H_1(\\Omega)^2$ tel que:\n",
    "$$f(x + u(x)) = g(x), \\forall x\\in \\Omega.$$\n",
    "\n",
    "\n",
    "Le problème inverse est mal posé: tout d'abord, l'existence d'une solution n'est pas garantie, et dans le cas où il existe une solution, on n'a pas nécessairement unicité de cette solution. Par exemple, si $f$ et $g$ sont des fonctions constantes, n'importe quel déplacement $u$ est solution\n",
    "\n",
    "Pour le résoudre, on se propose de le reformuler comme un problème d'optimisation: \n",
    "\n",
    "> $(P_2)$ On cherche une déformation $u$ du plan qui minimise:\n",
    "$$\n",
    "E(u)=\\displaystyle\\frac{1}{2}\\int_\\Omega (f(x+u(x))-g(x))^2 dx=\\frac{1}{2}\\|f\\circ (id+u)-g\\|^2.\n",
    "$$\n",
    "\n",
    "Sans hypothèse supplémentaire, le problème $\\displaystyle \\min_{u \\in H^1(\\Omega)^2} E(u)$ n'est a priori pas convexe, toujours mal posé et même éventuellement non différentiable si $u$ et $f$ ne sont pas assez régulières. On pourrait facilement rendre $f$ différentiable (en ajoutant du bruit par exemple à l'image, ce qui revient à convoluer $f$ avec une gaussienne) mais il faut également \"forcer\" $u$ à être différentiable. Pour cela on propose de régulariser le problème de façon à assurer la convexité du problème d'optimisation considéré ainsi que l'existence et l'unicité des solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour régulariser le problème inverse, nous allons faire une analogie avec l’élasticité linéaire. La fonction $u = (u_x,u_y)$ représente un champ de déformations. En notant $\\partial x$ et $\\partial y$ les opérateurs de dérivation partielle par rapport à chacun des axes du plan, on peut définir un potentiel élastique linéarisé :\n",
    "$$\n",
    "R(u)= \\frac{\\mu}{2}\\int_{\\Omega} \\underbrace{ (\\partial_x u_y + \\partial_y u_x)^2(x,y) dxdy}_{R_1(u)=\\textrm{cisaillement}} +\\frac{\\lambda+\\mu}{2}\\int_{\\Omega} \\underbrace{(\\partial_x u_x + \\partial_y u_y)^2(x,y) dxdy}_{R_2(u)=\\textrm{variations \\ de \\ volume}}.\n",
    "$$ \n",
    "En mécanique des structures, $\\mu$ et $\\lambda$ sont appelées constantes de Lamé. Le paramètre $\\lambda$ n’a pas d’interprétation directe, tandis que le paramètre $\\mu$ est appelé module de cisaillement.\n",
    "\n",
    "Le problème d'optimisation à résoudre dans ce TP est le suivant:\n",
    "> $$(P)\\qquad \\min_{u} E(u)+R(u).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. A l'aide d'un développement de Taylor, vérifier que le gradient de $E$ s'écrit:\n",
    "\n",
    "$$\\nabla E(u) = \\left(f\\circ (id+u) -g\\right)\\nabla f\\circ (id+u)$$\n",
    "\n",
    "au sens où la différentielle de $E$ est définie par:\n",
    "\n",
    "$$\\langle \\nabla E(u),h\\rangle = \\displaystyle\\int_\\Omega \\langle (f(x+u(x))-g(x))\\nabla f(x+u(x)),h(x)\\rangle dx.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait le développement de Taylor de $f\\displaystyle\\left(Id + u + h\\right)$ :\n",
    "\n",
    "$$\n",
    "f\\displaystyle\\left(Id + h + u\\right) = f\\displaystyle\\left(Id + u\\right) + \\nabla f\\displaystyle\\left(Id + u\\right) \\cdot h + o\\displaystyle\\left(h\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(u + h) = \\frac{1}{2}\\|f\\circ (id+u)-g\\|^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### __ICI FAUT CONTINUER__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Discrétisation\n",
    "\n",
    "Pour pouvoir résoudre numériquement le problème $(P)$ (dont les variables de l'optimisation sont des fonctions !), on propose de le discrétiser au préalable. \n",
    "\n",
    "Soit $1\\le i \\le n$ and $1\\le j\\le m$. Notons $(x_i,y_j)$ le point de la grille $(i,j)$ et $f_{i,j}$ la valeur de $f$ au point $(x_i,y_j)$. Le produit scalaire sur $V=\\mathbb{R}^n\\times \\mathbb{R}^m$ est défini par:\n",
    "$$\\langle f,g\\rangle_V=\\sum_{i=1}^n\\sum_{j=1}^m f_{i,j}g_{i,j},$$\n",
    "défini sur $\\mathbb{R}^n\\times \\mathbb{R}^m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Calcul du $E$ et de son gradient\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir calculer $E$ et son gradient, on va avoir besoin d'évaluer $f\\circ (Id+u)$ et $\\nabla f\\circ(id+u)$. C'est ce que fait la fonciton interpol ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpol(f,ux,uy) :\n",
    "    # function that computes f \\circ Id+u and interpolates it on a mesh\n",
    "    nx,ny=f.shape\n",
    "    ip=interpolate.RectBivariateSpline(np.arange(nx),np.arange(ny),f)\n",
    "    [X,Y]=np.meshgrid(np.arange(nx),np.arange(ny), indexing='ij')\n",
    "    X=X+ux\n",
    "    Y=Y+uy\n",
    "    return np.reshape(ip.ev(X.ravel(),Y.ravel()),(nx,ny))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Calcul de $R$ et de son gradient\n",
    "-------------------------------------\n",
    "On discrétise également les opérateurs de dérivation partielles par différences finies ; par exemple la dérivée partielle par rapport à $x$ est donnée par:\n",
    "$$\\begin{cases}(\\partial_x f)_{i,j}=f_{i+1,j}-f_{i,j} \\text{ si } i<n \\\\\n",
    "(\\partial_x f)_{n,j}=0 \\end{cases} $$\n",
    "\n",
    "On peut alors écrire :\n",
    "$$\n",
    "R(u)= \\frac{\\mu}{2}\\sum_{i}(\\partial_x u_y + \\partial_y u_x)^2(i) + \\frac{\\lambda+\\mu}{2} \\sum_i(\\partial_x u_x + \\partial_y u_y)^2(i).\n",
    "$$ \n",
    "où:\n",
    "* $u_x\\in\\mathbb{R}^n$ et $u_y\\in\\mathbb{R}^n$ sont les discrétisations des composantes du champ de vecteurs $u$ sur la grille choisie et $\\partial_x:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n$\n",
    "* $\\partial_y:\\mathbb{R}^n\\rightarrow\\mathbb{R}^n$ représentent des opérateurs de différences finies.\n",
    "\n",
    "On peut ré-écrire $R(u)=\\frac{1}{2}R_1(u)+ \\frac{1}{2} R_2(u)$ avec : \n",
    "$$\n",
    "R_1(u)=\\langle A_1 u , A_1 u\\rangle,\\qquad R_2(u)=\\langle A_2 u , A_2 u\\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Donner les formules de discrétisation des opérateurs $\\partial_y$, $\\partial_x^\\top$ et $\\partial_y^\\top$. Implémenter ces opérateurs ci-après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    d[:-1,:]=im[1:,:]-im[:-1,:]\n",
    "    return d\n",
    "def dy(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    # To be implemented -> implemented\n",
    "    d[:,:-1]=im[:,1:]-im[:,:-1]\n",
    "    return d\n",
    "def dyT(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    # To be implemented -> implemented\n",
    "    d[:,0]=-im[:,0]\n",
    "    d[:,-1]=im[:,-2]\n",
    "    d[:,1:-1]=im[:,0:-2]-im[:,1:-1]\n",
    "    return d  \n",
    "def dxT(im) :\n",
    "    d=np.zeros(im.shape)\n",
    "    # To be implemented\n",
    "    d[0,:]=-im[0,:]\n",
    "    d[-1,:]=im[-2,:]\n",
    "    d[1:-1,:]=im[0:-2,:]-im[1:-1,:]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. On écrit $R(u)$ comme un opérateur de $V^2$ dans $\\mathbb{R}$. Montrer que $R(u)$ peut s'écrire sous la forme:\n",
    "$$R(u)=\\frac{1}{2} \\langle A\\left(\\begin{array}{c}\n",
    "u_x\\\\\n",
    "u_y\n",
    "\\end{array}\\right),\\left(\\begin{array}{c}\n",
    "u_x\\\\\n",
    "u_y\n",
    "\\end{array}\\right)\\rangle_{V^2},$$\n",
    "et donnez l'expression des matrices $A_1$ et $A_2$ en fonction des opérateurs $\\partial_x$, $\\partial_x^\\top$, $\\partial_y$ et $\\partial_y^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. \n",
    "Sachant que l'on peut ré-écrire $R(u)=\\frac{1}{2}R_1(u)+ \\frac{1}{2} R_2(u)$ avec : \n",
    "$$\n",
    "R_1(u)=\\langle A_1 u , A_1 u\\rangle,\\qquad R_2(u)=\\langle A_2 u , A_2 u\\rangle.\n",
    "$$\n",
    "D'ou : $$R_1(u) = \\langle A_1 u , A_1 u\\rangle = ((A_1 u)^{T}(A_1 u)) = u^{T} A_1^{T} A_1  u $$\n",
    " $$   =((u^{T} A_1^{T} A_1 )^{T})^{T} u = ( A_1^{T} A_1  u)^{T} u = \\langle A_1^{T} A_1  u , u\\rangle  $$\n",
    "On a donc :\n",
    "$$R(u)=\\frac{1}{2}R_1(u)+ \\frac{1}{2} R_2(u) =  \\frac{1}{2} * (\\langle A_1 u , A_1 u\\rangle  + \\langle A_2 u , A_2 u\\rangle) = \\frac{1}{2} * (\\langle (A_1^{T} A_1) u\n",
    ", u\\rangle +  \\langle (A_2^{T} A_2) u, u\\rangle) =  \\frac{1}{2} *  \\langle (A_1^{T}  A_1 + A_2^{T} A_2) u, u\\rangle\n",
    "$$\n",
    "\n",
    "D'ou $$ A = A_1^{T}  A_1 + A_2^{T}  A_2 $$\n",
    "\n",
    "\n",
    "donnez l'expression des matrices $A_1$ et $A_2$ en fonction des opérateurs $\\partial_x$, $\\partial_x^\\top$, $\\partial_y$ et $\\partial_y^\\top$ : \n",
    "\n",
    "\n",
    "Sachant que l'on peut ré-écrire $R(u)=\\frac{1}{2}R_1(u)+ \\frac{1}{2} R_2(u)$ \n",
    "et que : $$R(u)= \\frac{\\mu}{2}\\sum{i}(\\partial_x u_y + \\partial_y u_x)^2(i) + \\frac{\\lambda+\\mu}{2} \\sum_i(\\partial_x u_x + \\partial_y u_y)^2(i)$$\n",
    "Prenons : \n",
    "$$R_1(u) = \\mu\\sum{i}(\\partial_x u_y + \\partial_y u_x)^2(i) $$\n",
    "Or sachant que : $$R_1(u)=\\langle A_1 u , A_1 u\\rangle,$$\n",
    "On a donc par identification: $$A_1 = \\sqrt{\\mu} * \\begin{bmatrix} \\partial_y  & \\partial_x  \\end{bmatrix}$$\n",
    "De manière analogue on trouve : \n",
    "$$A_2 = \\sqrt{(\\lambda+\\mu)} * \\begin{bmatrix} \\partial_x  & \\partial_y \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Donner l'expression du gradient de $R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "R(u+h) = \\, & \\, \\cfrac{1}{2} \\langle A \\begin{pmatrix} u_x + h_x \\\\ u_y + h_y \\end{pmatrix}, \\begin{pmatrix} u_x + h_x \\\\ u_y + h_y \\end{pmatrix} \\rangle _{V^2} \\\\\n",
    "= \\, & \\,\n",
    "\n",
    "\\underbrace{\n",
    "\\cfrac{1}{2}\n",
    "\\langle A \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}, \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix} \\rangle _{V^2}\n",
    "}_{R(u)}\n",
    "\n",
    "+ \\underbrace{\n",
    "\\cfrac{1}{2} \\displaystyle\\left[\n",
    "\\langle A \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}, \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix} \\rangle _{V^2}\n",
    "+ \\langle A \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix}, \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix} \\rangle _{V^2}\n",
    "\\displaystyle\\right]\n",
    "}_{\\langle \\nabla R(u) | h \\rangle}\n",
    "\n",
    "+ \\underbrace{\n",
    "\\cfrac{1}{2}\n",
    "\\langle A \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix}, \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix} \\rangle _{V^2}\n",
    "}_{o(h^2)}\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Et on a :\n",
    "\n",
    "$$\n",
    "\\cfrac{1}{2} \\displaystyle\\left[\n",
    "\\langle A \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix}, \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix} \\rangle _{V^2}\n",
    "+ \\langle A \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix}, \\begin{pmatrix} u_x \\\\ u_y \\end{pmatrix} \\rangle _{V^2}\n",
    "\\displaystyle\\right] \\, = \\, \n",
    "\\cfrac{1}{2} \\displaystyle\\left[\n",
    "\\begin{pmatrix} u_x & u_y \\end{pmatrix} A^{T} \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix} + \\begin{pmatrix} u_x & u_y \\end{pmatrix} A \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix}\n",
    "\\displaystyle\\right]\n",
    "$$\n",
    "\n",
    "Or, on a construit la matrice $A$ de la manière suivante :\n",
    "\n",
    "$$\n",
    "A = A_1^{T}A_1 + A_2^{T}A_2\n",
    "$$\n",
    "\n",
    "Cette matrice est symétrique, d'où :\n",
    "\n",
    "$$\n",
    "A^{T} = A\n",
    "$$\n",
    "\n",
    "Et ainsi :\n",
    "\n",
    "$$\n",
    "\\langle \\nabla R(u) | h \\rangle = \\begin{pmatrix} u_x & u_y \\end{pmatrix} A^{T} \\begin{pmatrix} h_x \\\\ h_y \\end{pmatrix} = \\langle Au | h \\rangle\n",
    "$$\n",
    "\n",
    "Finalement :\n",
    "\n",
    "$$\n",
    "\\nabla R(u) = Au\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Implémentation de la fonction objectif $E+R$\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(f,g,ux,uy,lamb,mu) :\n",
    "    raise ValueError('to be implemented') \n",
    "\n",
    "\n",
    "    #implementation de E(u)+R(u)\n",
    "    fu = interpol(f,ux,uy)\n",
    "    \n",
    "    #Calcul de R\n",
    "    R = mu/2 * npl.norm(dx(uy)+dy(ux))**2 + (lamb + mu)/2 * npl.norm(dx(ux) + dy(uy))**2\n",
    "    \n",
    "    #Calcul de E\n",
    "    E = 1/2 * npl.norm(fu - g)**2 \n",
    "\n",
    "\n",
    "\n",
    "    obj = E + R\n",
    "    return obj,fu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Un algorithme de gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une itération de la méthode de descente de gradient est de la forme:\n",
    "\n",
    "$$\n",
    "u_{k+1}=u_k-s_k(\\nabla E(u) + \\nabla R(u))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Compléter la fonction RecalageDG implémentant la descente de gradient et utilisant l'algorithme de recherche linéaire par rebroussement proposé ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(ux,uy,step,descentx,descenty,obj_old,f,g,lamb,mu) :\n",
    "    step=2*step\n",
    "    tmpx=ux-step*descentx\n",
    "    tmpy=uy-step*descenty\n",
    "    obj,fu=objective_function(f,g,tmpx,tmpy,lamb,mu)\n",
    "    while obj >obj_old and step > 1.e-8:\n",
    "        step=0.5*step\n",
    "        tmpx=ux-step*descentx\n",
    "        tmpy=uy-step*descenty\n",
    "        obj,fu=objective_function(f,g,tmpx,tmpy,lamb,mu)\n",
    "    return tmpx,tmpy,step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecalageDG(f,g,lamb,mu,nitermax,stepini) : \n",
    "    ux=np.zeros(f.shape)\n",
    "    uy=np.zeros(f.shape)  \n",
    "    CF=[]\n",
    "    step_list=[]\n",
    "    niter=0\n",
    "    step=stepini\n",
    "    while niter < nitermax and step > 1.e-8 : \n",
    "        niter+=1\n",
    "        obj,fu=objective_function(f,g,ux,uy,lamb,mu)\n",
    "        CF.append(obj)   \n",
    "        # Gradient of E at point u\n",
    "        #raise ValueError('Compute gradEx and gradEy here')\n",
    "        gradEx=(fu-g)*(interpol(dfx,ux,uy))\n",
    "        gradEy=(fu-g)*(interpol(dfy,ux,uy))\n",
    "        \n",
    "        # Gradient of R at point u\n",
    "        #raise ValueError('Compute gradRx and gradRy here')\n",
    "        gradRx = mu * (dyT(dy(ux)) + dyT(dx(uy))) + (lamb + mu) * (dxT(dx(ux)) + dxT(dy(uy)))\n",
    "        gradRy = mu * (dxT(dy(ux)) + dxT(dx(uy))) + (lamb + mu) * (dyT(dx(ux)) + dyT(dy(uy)))\n",
    "        \n",
    "        # Gradient of E+R at point u\n",
    "        #raise ValueError('Compute gradx and grady here')\n",
    "        gradx = gradEx + gradRx\n",
    "        grady = gradEy + gradRy\n",
    "        \n",
    "        ux,uy,step=linesearch(ux,uy,step,gradx,grady,obj,f,g,lamb,mu)\n",
    "        step_list.append(step)\n",
    "        if (niter % 3 ==0) :\n",
    "            print('iteration :',niter,' cost function :',obj,'step :',step)\n",
    "    return ux,uy,np.array(CF),np.array(step_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Ecrire un compte-rendu des expériences réalisées et des résultats obtenus. Commentez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb=10\n",
    "mu=20\n",
    "nitermax=500\n",
    "\n",
    "step0 = 0.01\n",
    "ux,uy,CF,step=RecalageDG(f,g,lamb,mu,nitermax,step0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "ax[0,0].imshow(f)\n",
    "ax[0,0].set_title('original function')\n",
    "ax[0,1].imshow(g)\n",
    "ax[0,1].set_title('target function')\n",
    "ax[1,0].quiver(ux,uy)\n",
    "ax[1,0].set_title('displacement field')\n",
    "ax[1,1].imshow(interpol(f,ux,uy))\n",
    "ax[1,1].set_title('final function')\n",
    "ax[0,2].plot(CF)\n",
    "ax[0,2].set_title('objective history')\n",
    "ax[1,2].plot(np.log(step))\n",
    "ax[1,2].set_title('step history (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(ux, uy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Algorithme de moindres carrés.\n",
    "\n",
    "On souhaite maintenant implémenter un algorithme de second ordre pour résoudre le problème $(P)\\quad\\min_u E(u)+R(u)$ afin d'accélérer la convergence de l'algorithme. Pour cela, on va reformuler le problème $(P)$ en un problème de moindres carrés et appliquer l'algorithme de Levenberg-Marquardt.\n",
    "\n",
    "Soit:\n",
    "$$\\Psi(u)=\\begin{pmatrix} \n",
    "f\\circ(Id+u)-g \\\\ \n",
    "\\sqrt{\\mu}(\\partial_xu_y+\\partial_yu_x) \\\\ \n",
    "\\sqrt{\\mu+\\lambda}(\\partial_xu_x+\\partial_yu_y) \\end{pmatrix},$$\n",
    "où $f\\circ(id+u)$ est l'interpolation de $x\\mapsto f(x+u(x))$ sur la grille. Minimiser $E(u)+R(u)$ est équivalent à résoudre le problème suivant:\n",
    "\n",
    ">$$\\min_u \\|\\Psi(u)\\|_2^2.$$\n",
    "\n",
    "Il s'agit maintenant d'un problème de moindres carrés que l'on va résoudre à l'aide de l'algorithme de Levenberg Marquardt :\n",
    "\n",
    "$$\n",
    "u_{k+1}=u_k- H_k^{-1} J_{\\Psi}(u_k)^\\top \\Psi(u_k) \\quad\\text{ avec }\\quad H_k=J_{\\Psi}(u_k)^\\top J_{\\Psi}(u_k) +\\varepsilon Id\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Calculer la matrice jacobienne de $\\Psi$, notée $J_\\Psi(u)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Implémenter les fonctions JPsi, JTPsi et JTJ qui calculent respectivement:\n",
    "- le produit de $J_\\psi(u)$ par une direction $v=(v_x,v_y)\\in V^2$,\n",
    "\n",
    "- le produit de $J_\\Psi(u)^\\top$ par $\\phi=(\\phi_1,\\phi_2,\\phi_3)\\in V^3$,\n",
    "\n",
    "- le produit de $(J_\\Psi(u)^\\top J_\\Psi(u)+\\epsilon I)$ par une direction $v=(v_x,v_y)\\in V^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JTPsi(phi,df,lamb,mu) :\n",
    "    raise ValueError('to be implemented') \n",
    "    #return [ux,uy]\n",
    "  \n",
    "def JPsi(vx,vy,df,lamb,mu) :\n",
    "    raise ValueError('to be implemented') \n",
    "    #return [JPsi0,JPsi1,JPsi2]\n",
    "  \n",
    "def JTJ(vx,vy,df,lamb,mu,epsilon) :\n",
    "    raise ValueError('to be implemented') \n",
    "    #return uxs,uys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons maintenant la direction de recherche $d_k$ comme solution du système linéaire:\n",
    "$$(J_\\Psi(u_k)^\\top J_\\Psi(u_k) +\\epsilon I)\\left(\\begin{array}{c}\n",
    "d_x\\\\\n",
    "d_y\n",
    "\\end{array}\\right) = -J_\\Psi(u_k)^\\top \\Psi(u_k)$$\n",
    "Pour cela, on vous donne l'algorithme suivant qui par la méthode du gradient conjugué calcule une solution $d=(d_x,d_y)\\in V^2$ du problème:\n",
    "$$(J_\\Psi(u_k)^\\top J_\\Psi(u_k) +\\epsilon I)\\left(\\begin{array}{c}\n",
    "d_x\\\\\n",
    "d_y\n",
    "\\end{array}\\right) = b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def CGSolve(u0x,u0y,lamb,mu,b,epsilon,df) :\n",
    "    nitmax=100;\n",
    "    ux=u0x; uy=u0y; #point de départ de l'algorithme\n",
    "    # Computes JTJu\n",
    "    Ax,Ay=JTJ(ux,uy,df,lamb,mu,epsilon);\n",
    "    rx=b[0]-Ax\n",
    "    ry=b[1]-Ay\n",
    "    px=rx\n",
    "    py=ry\n",
    "    rsold=np.linalg.norm(rx)**2+np.linalg.norm(ry)**2\n",
    "    for i in range(nitmax) :\n",
    "        Apx,Apy=JTJ(px,py,df,lamb,mu,epsilon);\n",
    "        alpha=rsold/(np.vdot(rx[:],Apx[:])+np.vdot(ry[:],Apy[:]))\n",
    "        ux=ux+alpha*px\n",
    "        uy=uy+alpha*py\n",
    "        rx=rx-alpha*Apx\n",
    "        ry=ry-alpha*Apy\n",
    "        rsnew=np.linalg.norm(rx)**2+np.linalg.norm(ry)**2\n",
    "        if np.sqrt(rsnew)<1e-10 :\n",
    "            return [ux,uy]\n",
    "        px=rx+rsnew/rsold*px\n",
    "        py=ry+rsnew/rsold*py\n",
    "        rsold=rsnew\n",
    "    return [ux,uy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Compléter l'algorithme RecalageGN implémentant la méthode de Levenberg-Marquardt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecalageGN(f,g,lamb,mu,nitermax,stepini,epsi) : \n",
    "    ux=np.zeros(f.shape)\n",
    "    uy=np.zeros(f.shape)  \n",
    "    descentx=np.zeros(f.shape)\n",
    "    descenty=np.zeros(f.shape)  \n",
    "    raise ValueError('To complete if necessary')\n",
    "    CF=[]\n",
    "    step_list=[]\n",
    "    niter=0\n",
    "    step=stepini\n",
    "    while niter < nitermax and step > 1.e-8 : \n",
    "        niter+=1\n",
    "        obj,fu=objective_function(f,g,ux,uy,lamb,mu)\n",
    "        CF.append(obj)\n",
    "        # Gradient of F at point u\n",
    "        raise ValueError('Compute b here')\n",
    "        raise ValueError('Compute dfx,dfy here')    \n",
    "        [descentx,descenty]=CGSolve(descentx,descenty,lamb,mu,b,epsi,dfx,dfy)\n",
    "        ux,uy,step=linesearch(ux,uy,step,descentx,descenty,obj,f,g,lamb,mu)\n",
    "        step_list.append(step)\n",
    "        # Display\n",
    "        if (niter % 3 ==0) :\n",
    "            print('iteration :',niter,' cost function :',obj,'step :',step)\n",
    "    return ux,uy,np.array(CF),np.array(step_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Tester le nouvel algorithme et comparer sa vitesse de convergence avec celle de l'algorithme de gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsi=0.1\n",
    "nitermax=1000\n",
    "ux,uy,CF,step=RecalageGN(f,g,lamb,mu,nitermax,step0,epsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3)\n",
    "ax[0,0].imshow(f)\n",
    "ax[0,0].set_title('original function')\n",
    "ax[0,1].imshow(g)\n",
    "ax[0,1].set_title('target function')\n",
    "ax[1,0].quiver(ux,uy)\n",
    "ax[1,0].set_title('displacement field')\n",
    "ax[1,1].imshow(interpol(f,ux,uy))\n",
    "ax[1,1].set_title('final function')\n",
    "ax[0,2].plot(CF)\n",
    "ax[0,2].set_title('objective history')\n",
    "ax[1,2].plot(np.log(step))\n",
    "ax[1,2].set_title('step history (log scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.quiver(ux, uy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Jeu des différences\n",
    "======================\n",
    "Maintenant que vous avez implémenté et testé les deux algorithmes sur l'image-jouet proposée, voyons que cela donne sur une image IRM d'un cerveau. Saurez-vous détecter les différences/déplacements entre les deux images ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1=Image.open('IRM1.png')\n",
    "im2=Image.open(\"IRM2.png\")\n",
    "plt.imshow(plt.imread('IRM1.png'))\n",
    "plt.show()\n",
    "plt.imshow(plt.imread('IRM2.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n,m]=im1.size\n",
    "sigma=0.1\n",
    "[X,Y]=np.meshgrid(np.linspace(-1,1,n),np.linspace(-1,1,m), indexing='xy')\n",
    "Z=np.sqrt(X*X+Y*Y)\n",
    "G=np.fft.fftshift(np.exp(-(X**2+Y**2)/sigma**2))\n",
    "f=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im1)))\n",
    "g=np.real(np.fft.ifft2(np.fft.fft2(G)*np.fft.fft2(im2))) \n",
    "f=f/np.max(f)\n",
    "g=g/np.max(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modia_py_mathsapp",
   "language": "python",
   "name": "modia_py_mathsapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

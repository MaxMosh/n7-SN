{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports classiques\n",
    "Nous allons tout d'abord lancer les imports classiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Optim as opt\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contour_and_f_plot(res,function, levels=None,xmin=-2,xmax=2,ymin=-2,ymax=2):\n",
    "    xiter=np.array(res['list_x'])\n",
    "    fig, axarr = plt.subplots(2, 2, figsize=(16,8))\n",
    "    # First plot \n",
    "    axarr[0,0].set_title('Points and levelset')\n",
    "    Nx = 1000\n",
    "    Ny = 1000\n",
    "    x = np.linspace(xmin,xmax,Nx)\n",
    "    y = np.linspace(ymin,ymax,Ny)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z=function.value((X,Y))\n",
    "    if levels:\n",
    "        CS = axarr[0,0].contour(X, Y, Z, levels)\n",
    "    else:\n",
    "        CS = axarr[0,0].contour(X, Y, Z)\n",
    "    axarr[0,0].plot(xiter[:,0], xiter[:,1],'+')\n",
    "    axarr[0,0].clabel(CS, inline=1, fontsize=10)\n",
    "    axarr[0,0].axis('equal')\n",
    "    # Second plot\n",
    "    axarr[0,1].set_title('Evolution of the cost')\n",
    "    fiter=np.array(res['list_costs'])\n",
    "    if min(fiter) > 0:\n",
    "        axarr[0,1].semilogy(fiter)\n",
    "    else:\n",
    "        axarr[0,1].plot(fiter)\n",
    "    #Third plot\n",
    "    axarr[1,0].set_title('Norm of the gradient')\n",
    "    giter=np.array(res['list_grads'])\n",
    "    axarr[1,0].semilogy(giter)\n",
    "    #Fourth plot\n",
    "    axarr[1,1].set_title('Steps')\n",
    "    siter=np.array(res['list_steps'])\n",
    "    axarr[1,1].plot(siter)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS\n",
    "Nous allons nous intéresser à l'algorithme Limited Memory BFGS. Cet algorithme est du type BFGS, c'est à dire qu'il estime l'inverse de la Hessienne de $f$. Le L dans le nom de l'algorithme signifie qu'il est à mémoire limitée, c'est à dire qu'il ne garde en mémoire que les $L$ dernières itérations de calcul pour estimer la Hessienne.\n",
    "L'algorithme est le suivant : Nous sommes à l'itération $k$, nous notons $x_k$ l'itéré et nous avons stocké les vecteurs suivants pour tout $k_{min}\\le i\\le k$.\n",
    "$$ \\sigma_i=x_{i}-x_{i-1} \\text{ et } y_i=\\nabla f(x_{i}) -\\nabla f(x_{i-1})$$\n",
    "Et on a aussi stocké $\\rho_i=\\frac{1}{(\\sigma_i,y_i)}$. Tous les $\\rho_i$ doivent être positifs.\n",
    "L'algorithme est le suivant \n",
    "\n",
    "<div style=\"background-color:rgba(0, 0, 0, 0.0470588); padding:10px 0;font-family:monospace;\">\n",
    "$q=-\\nabla f(x_k)$<br>\n",
    "Pour $i=k,k-1,\\dots k_{min}$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\alpha_i=\\rho_i(\\sigma_i \\cdot q)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$q=q-\\alpha_i y_i$<br>\n",
    "$r=\\displaystyle \\frac{(\\sigma_{k_{min}}\\cdot y_{k_{min}})}{(y_{k_{min}}\\cdot y_{k_{min}})}r$<br>\n",
    "Pour $i=k_{min},k_{min}+1,\\dots,k$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$\\beta_i=\\rho_i(y_i\\cdot r)$<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;$q=q+(\\alpha_i-\\beta_i)\\sigma_i$<br>\n",
    "rend r\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "# Fonctions sur les listes\n",
    "Vous aurez sans doute besoin des fonctions suivantes pour les listes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[(2*i,3*i) for i in range(5) ]\n",
    "print(a)\n",
    "print(a[3],a[-1])\n",
    "print('*** Pop ***')\n",
    "a.pop(0)\n",
    "print(a)\n",
    "print('*** Parcours ***')\n",
    "for e,f in a :\n",
    "      print(e,'et',f)\n",
    "print('*** Parcours Inverse***')\n",
    "for e in reversed(a) :\n",
    "      print(e)\n",
    "b=[e**2 for e,f in a]\n",
    "print(b)\n",
    "print('*** Parcours de deux listes ensembles***')\n",
    "for (m,(t,p)) in zip(b,a) :\n",
    "    print(m,'et',t,'et encore',p)\n",
    "print('*** Append ***')\n",
    "print(b)\n",
    "b.append(546)\n",
    "print(b)\n",
    "print('*** Inversion ***')\n",
    "c=list(reversed(b))\n",
    "print(c)\n",
    "print(len(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class BFGS\n",
    "Créez une classe `BFGS` dans `Optim.py`, sa fonction `__init__` sera de la forme \n",
    "`__init__(self,nb_stock_max=8)` où `nb_stock_max` est le nombre maximum d'itérations prises en compte. Cette fonction créera aussi une liste vide appelée `stock` qui conserve les $\\sigma_i,y_i,\\rho_i$. Elle devra aussi créer une liste vide nommée `last_iter`.\n",
    "\n",
    "\n",
    "# Push\n",
    "Nous allons maintenant créer une fonction `push(self, x, grad)` qui enregistre $s\\sigma_k,y_k,\\rho_k$. Pour cela, on a besoin de $x_{k-1},\\nabla f(x_{k-1})$. Si ils existent, ils se trouvent dans la liste `self.last_iter`. Ensuite on peut calculer $\\sigma_k,y_k$ et $\\rho_k$. \n",
    "\n",
    "Si $\\rho_k$ est positif, alors on enregistre le triplet $(\\sigma_k,y_k,\\rho_k)$ à la fin de la liste `self.stock`, en vérifiant `self.stock` ne doit contenir au maximum que les dernières `self.nb_stock_max` itérations (si nécessaire on retire le tout premier élément de `self.stock`). \n",
    "\n",
    "Si $\\rho_k$ est négatif, quelquechose c'est mal passé, on vide le `self.stock`.\n",
    "\n",
    "A la fin, on n'oublie pas de mettre $x_{k}$ et $\\nabla f(x_{k})$ dans `self.last_iter` pour être sûr de les y trouver la prochaine fois.\n",
    "\n",
    "# Get\n",
    "\n",
    "Nous allons maintenant créer une fonction `get(self, grad)` qui modifie la direction de descente et applique l'algorithme ci-dessus. Cette fonction doit nous rendre le `r` final. Si le `self.stock` est vide, cette fonction doit nous rendre `-grad`\n",
    "\n",
    "# dc\n",
    "\n",
    "Nous créeons maintenant une fonction `dc(self,x,function,df)` qui appplique tout d'abord `self.push` puis `self.get`, elle rend le résultat de la fonction de `self.get` et une variable d'info.\n",
    "\n",
    "# C'est l'heure de tester ...\n",
    "Lancez une méthode de Newton_Wolfe sur votre fonction préférée et à chaque itération calculez ce que donnerait un L-BFGS. Comparez les angles des directions entre la méthode de Newton et le L-BFGS, comparez aussi le ration des normes. Ensuite lancez un LBFGS avec recherche de pas de Wolfe sur vos tests préférez et obtenez le comportement de Newton_Wolfe sans le calcul de la Hessienne...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=func.Rosen()\n",
    "x0=np.array([-1,-1])\n",
    "res=opt.main_algorithm(f,0.1,x0,ls=opt.ls_wolfe_step_is_one,dc=opt.dc_Newton,verbose=True)\n",
    "contour_and_f_plot(res,f)\n",
    "print(res['dc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=func.Rosen()\n",
    "B=opt.BFGS(nb_stock_max=12)\n",
    "x0=np.array([-1,-1])\n",
    "res=opt.main_algorithm(f,0.1,x0,ls=opt.ls_wolfe_step_is_one,dc=B.dc,verbose=True)\n",
    "contour_and_f_plot(res,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "import xicorpy                  # Pour utiliser les estimateurs des indices de Cramèr-von-Mises\n",
    "from sklearn import tree        # Pour utiliser les arbres de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour éviter quelques erreurs d'affichages suite à la conversion numpy/pandas en utilisant le package xicorpy\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1) Classification sans l'Analyse de Sensibilité**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons travailler avec le modèle théorique :\n",
    "$$ Y =  X^{(1)} \\times \\dots \\times X^{(d)} $$\n",
    "\n",
    "Nous cherchons à classifier selon le signe de $Y$. Il y a donc trois classes : $\\left\\{-1, \\ 0, \\ 1\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeSimulator( dim_d ):\n",
    "    return lambda inputs : np.prod( inputs[ : dim_d, : ], axis = 0 ) \n",
    "\n",
    "dim_d     = 5\n",
    "simulator = MakeSimulator( dim_d )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons noyer les $d$ variables dans $D \\ge d$ variables indépendantes. Chacun des $X^{(i)}$ sera uniforme sur $[0,1]$. Regardons d'abord ce qu'il se passe pour $d=D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_D = 5\n",
    "\n",
    "n_train = int( 7.5e4 )\n",
    "n_test  = int( 2.5e4 )\n",
    "\n",
    "inputs_train = np.random.uniform( low = -1, high = 1, size = ( dim_D, n_train ) )\n",
    "inputs_test  = np.random.uniform( low = -1, high = 1, size = ( dim_D, n_train ) )\n",
    "output_train = simulator( inputs_train )\n",
    "output_test  = simulator( inputs_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Utiliser scikit-learn pour importer un algorithme générant un arbre de classification. L'utiliser pour entrainer un arbre sur les données d'entrainement, puis calculer le score du modèle sur les données de test.\n",
    "(voir https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Faire maintenant varier le nombre de variable totale de $D=5$ à $D=20$. Observer le score du classifieur sur un scatter plot, à mesure que $D$ augmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_D_max = 20\n",
    "score_list_no_gsa = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10, 5 ) )\n",
    "plt.scatter( np.arange( dim_d, dim_D_max ), score_list_no_gsa, color = 'blue', label = 'Score du Classifieur' )\n",
    "plt.title( 'Score du classifieur quand la dimension augmente', fontsize = 16 )\n",
    "plt.xlabel( u'Nombre de variable au total $D$', fontsize = 14) \n",
    "plt.ylabel( u'Score $R^2$', fontsize = 14 )\n",
    "plt.legend( fontsize = 10 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Le score pour des \"grandes\" valeurs de $D$ est-il prévisible, ou du moins explicable ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Produire un scatter plot représentant le temps d'entrainement à mesure que le nombre de variable augmente. On prendra de nouveau $D$ entre 5 et 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_D_max = 20\n",
    "time_list_no_gsa = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10, 5 ) )\n",
    "plt.scatter( np.arange( dim_d, dim_D_max ), time_list_no_gsa, color = 'blue', label = \"Temps d'entrainement\" )\n",
    "plt.title( \"Temps d'entrainement du classifieur quand la dimension augmente\", fontsize = 16 )\n",
    "plt.xlabel( u'Nombre de variable au total $D$', fontsize = 14) \n",
    "plt.ylabel( u'Temps en seconde', fontsize = 14 )\n",
    "plt.legend( fontsize = 10 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2) Intervention de l'Analyse de Sensibilité**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser l'estimation des indices de Cramèr-von-Mises pour réduire le nombre de variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Revenir au cas $d = D = 5$ et évaluer les indices de Cramèr-von-Mises pour chaque variable.\n",
    "(voir https://swarnakumar.github.io/xicorpy/xi/ et pour plus de détail sur l'api : https://swarnakumar.github.io/xicorpy/xicor_api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Quelle information semble ressortir de cette analyse à propos des effets d'ordre supérieur ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle que d'après le TCL pour l'estimateur $\\widehat{S}_{\\mathrm{CvM}}^{(i)}$ des indices de CvM, sous l'hypothèse que $X^{(i)}$ est indépendante de $$Y$, on a :\n",
    "$$ \\sqrt{ \\frac{5n}{2} }\\widehat{S}_{\\mathrm{CvM}}^{(i)} \\rightarrow \\mathcal{N}(0,1) $$\n",
    "Ainsi si la réalisation de $\\widehat{S}_{\\mathrm{CvM}}^{(i)}$ dépasse $1.96 \\times \\sqrt{ \\frac{2}{5n} }$, alors on peut considérer la variable $X^{(i)}$ pertinente avec un risque de $5\\%$ (test unilatéral)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Reprendre la classifcation et le calcul de score effectué à la question 2, en sélectionnant uniquement les variables, dont l'estimateur des indices de CvM dépassent le seuil donné ci-dessus par le TCL. En faire aussi un scatter plot et le comparer à celui établi à la question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_D_max = 20\n",
    "score_list_gsa = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10, 5 ) )\n",
    "plt.scatter( np.arange( dim_d, dim_D_max ), score_list_no_gsa, color = 'blue', label = 'Sans Analyse de Sensibilité' )\n",
    "plt.scatter( np.arange( dim_d, dim_D_max ), score_list_gsa, color = 'red', label = 'Avec Analyse de Sensibilité' )\n",
    "plt.title( 'Score du classifieur quand la dimension augmente', fontsize = 16 )\n",
    "plt.xlabel( u'Nombre de variable au total $D$', fontsize = 14) \n",
    "plt.ylabel( u'Score $R^2$', fontsize = 14 )\n",
    "plt.legend( fontsize = 10 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Reprendre la classifcation et l'évaluation du temps d'entrainement effectué à la question 4, en sélectionnant uniquement les variables, dont l'estimateur des indices de CvM dépassent le seuil donné ci-dessus par le TCL. En faire aussi un scatter plot et le comparer à celui établi à la question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_D_max = 20\n",
    "time_list_gsa = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize = ( 10, 5 ) )\n",
    "plt.scatter( np.arange( dim_d, dim_D_max ), time_list_no_gsa, color = 'blue', label = 'Sans Analyse de Sensibilité' )\n",
    "plt.scatter( np.arange( dim_d, dim_D_max ), time_list_gsa, color = 'red', label = 'Avec Analyse de Sensibilité' )\n",
    "plt.title( \"Temps d'entrainement du classifieur quand la dimension augmente\", fontsize = 16 )\n",
    "plt.xlabel( u'Nombre de variable au total $D$', fontsize = 14) \n",
    "plt.ylabel( u\"Temps d'entrainement en seconde\", fontsize = 14 )\n",
    "plt.legend( fontsize = 10 )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
